{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9f9b55a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "try:\n",
    "    import cupy as cp\n",
    "    import cupy.cuda.runtime as runtime\n",
    "\n",
    "    # get the num of GPUs\n",
    "    n_gpus = runtime.getDeviceCount()\n",
    "    print(f\"Detected {n_gpus} GPU(s):\")\n",
    "    cupy_gpu_id = 7\n",
    "    cp.cuda.Device(cupy_gpu_id).use()\n",
    "    print(f\"Using GPU {cupy_gpu_id}\")\n",
    "    using_cupy = True\n",
    "except ImportError:\n",
    "    print(\"CuPy is not available, falling back to NumPy.\")\n",
    "    using_cupy = False\n",
    "    cp = np  # rename cupy to np for compatibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6df3c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import create_train_test_group\n",
    "# update mroRnnDataset with better performance\n",
    "from utils import mroRnnDatasetNew as mroRnnDataset\n",
    "from utils import collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dddc0d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if CUDA (NVIDIA GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available CUDA devices\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_gpus}\")\n",
    "\n",
    "    # Select a specific GPU (e.g., GPU 0)\n",
    "    device = torch.device(\"cuda:8\")  # Use \"cuda:1\" for GPU 1, etc.\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(device)}\")\n",
    "\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA is not available, using CPU\")\n",
    "\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c200249e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_frac = 0.01\n",
    "test_size = 0.1\n",
    "max_seq_length = 10\n",
    "batch_size = 4096\n",
    "num_workers = 16\n",
    "rnn_output_size = 16\n",
    "learning_rate = 0.01\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "counter = 0\n",
    "model_save_folder = \"./Out\"\n",
    "model_name = \"Apr_28_LSTM_feature_eng\"\n",
    "patience = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618c5bd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = \"./Data/mro_daily_clean.csv\"\n",
    "data = pd.read_csv(file_name, index_col=0, engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20ca21da",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_need_std = [\n",
    "    \"hard_braking\",\n",
    "    \"hard_acceleration\",\n",
    "    \"speeding_sum\",\n",
    "    \"day_mileage\",\n",
    "    \"engn_size\",\n",
    "    \"est_hh_incm_prmr_cd\",\n",
    "    \"purchaser_age_at_tm_of_purch\",\n",
    "    \"tavg\",\n",
    "    \"random_avg_traffic\",\n",
    "]\n",
    "\n",
    "column_after_std = [\n",
    "    \"hard_braking_std\",\n",
    "    \"hard_acceleration_std\",\n",
    "    \"speeding_sum_std\",\n",
    "    \"day_mileage_std\",\n",
    "    \"engn_size_std\",\n",
    "    \"est_hh_incm_prmr_cd_std\",\n",
    "    \"purchaser_age_at_tm_of_purch_std\",\n",
    "    \"tavg_std\",\n",
    "    \"random_avg_traffic_std\",\n",
    "]\n",
    "\n",
    "column_need_encode = [\n",
    "    \"gmqualty_model\",\n",
    "    \"umf_xref_finc_gbl_trim\",\n",
    "    \"input_indiv_gndr_prmr_cd\",\n",
    "]\n",
    "\n",
    "column_after_encode = [\n",
    "    \"gmqualty_model_encode\",\n",
    "    \"umf_xref_finc_gbl_trim_encode\",\n",
    "    \"input_indiv_gndr_prmr_cd_encode\",\n",
    "]\n",
    "\n",
    "column_after_encode_std = [\n",
    "    \"gmqualty_model_encode_std\",\n",
    "    \"umf_xref_finc_gbl_trim_encode_std\",\n",
    "    \"input_indiv_gndr_prmr_cd_encode_std\",\n",
    "]\n",
    "\n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "data[column_after_std] = scaler.fit_transform(data[column_need_std])\n",
    "\n",
    "# encode data\n",
    "label_encoders = {}\n",
    "for i, col in enumerate(column_need_encode):\n",
    "    le = LabelEncoder()\n",
    "    data[column_after_encode[i]] = le.fit_transform(data[col])\n",
    "    label_encoders[col] = le  # Store the encoder for later use if needed\n",
    "\n",
    "data[column_after_encode_std] = scaler.fit_transform(data[column_after_encode])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1d71b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage (assuming 'data', 'column_after_std', and 'column_after_encode_std' are already defined):\n",
    "col_rnn_origin = [\"id\"] + column_after_std + column_after_encode_std + [\"mro\"]\n",
    "\n",
    "data_rnn_origin = data[\n",
    "    col_rnn_origin\n",
    "].copy()  # Create a copy to avoid modifying the original DataFrame\n",
    "\n",
    "data_rnn_origin = create_train_test_group(\n",
    "    data_rnn_origin, sample_frac=sample_frac, test_size=test_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "482c6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_features = [\n",
    "    \"hard_braking_std\",\n",
    "    \"hard_acceleration_std\",\n",
    "    \"speeding_sum_std\",\n",
    "    \"day_mileage_std\",\n",
    "    \"engn_size_std\",\n",
    "    \"est_hh_incm_prmr_cd_std\",\n",
    "    \"purchaser_age_at_tm_of_purch_std\",\n",
    "    \"tavg_std\",\n",
    "    \"random_avg_traffic_std\",\n",
    "    \"gmqualty_model_encode_std\",\n",
    "    \"umf_xref_finc_gbl_trim_encode_std\",\n",
    "    \"input_indiv_gndr_prmr_cd_encode_std\",\n",
    "]\n",
    "\n",
    "rnn_target = [\"mro\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c45d0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"train\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "test_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"test\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614e111b",
   "metadata": {},
   "outputs": [],
   "source": [
    "  # Example batch size\n",
    "train_dataloader = DataLoader(\n",
    "    train_data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dab055d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for input, target, length in train_dataloader:\n",
    "#     target: torch.Tensor\n",
    "#     input: torch.Tensor\n",
    "#     print(input.shape)\n",
    "#     print(target.shape)\n",
    "#     print(length)\n",
    "#     break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd917b97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RnnModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type: str,\n",
    "        input_size,\n",
    "        rnn_output_size,\n",
    "        output_size,\n",
    "    ):\n",
    "        super(RnnModel, self).__init__()\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=rnn_output_size,\n",
    "                num_layers=3,\n",
    "                batch_first=True,\n",
    "                bidirectional=True,\n",
    "                dropout=0.5,\n",
    "            )\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(rnn_output_size*2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, output_size),\n",
    "        )\n",
    "        # self.fc = nn.Linear(rnn_output_size, output_size)\n",
    "\n",
    "    def forward(self, x, length: torch.Tensor):\n",
    "        # x  (batch_size, seq_len, input_size)\n",
    "        # use pack_padded_sequence and pad_packed_sequence to deal with different length of x input\n",
    "        packed_x = pack_padded_sequence(\n",
    "            x, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.rnn(packed_x)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        model_out = self.fc(rnn_out)  # (batch_size, seq_len, output_size)\n",
    "        return model_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e3db69",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_size = len(rnn_features)\n",
    "output_size = len(rnn_target)\n",
    "\n",
    "model = RnnModel(\n",
    "    rnn_type=\"LSTM\",\n",
    "    input_size=input_feature_size,\n",
    "    rnn_output_size=rnn_output_size,\n",
    "    output_size=output_size,\n",
    ").to(device)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d78bb9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight_value = (data_rnn_origin[\"mro\"] == 0).sum() / (data_rnn_origin[\"mro\"] == 1).sum()\n",
    "print(f\"pos_weight: {pos_weight_value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b514a1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# criterion = nn.BCELoss()\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_value])).to(device)\n",
    "# criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9dacdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "log_dir = \"Out\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file_path = os.path.join(log_dir, f\"training_log_{timestamp}.csv\")\n",
    "\n",
    "fieldnames = [\n",
    "    \"Epoch\",\n",
    "    \"Train Loss\",\n",
    "    \"Train F1\",\n",
    "    \"Train Accuracy\",\n",
    "    \"Train Recall\",\n",
    "    \"Train Precision\",\n",
    "    \"Val Loss\",\n",
    "    \"Val F1\",\n",
    "    \"Val Accuracy\",\n",
    "    \"Val Recall\",\n",
    "    \"Val Precision\",\n",
    "]\n",
    "\n",
    "log_df = pd.DataFrame(columns=fieldnames)\n",
    "log_df.to_csv(log_file_path, index=False)\n",
    "\n",
    "pt_model_dir = os.path.join(log_dir, \"models\")\n",
    "os.makedirs(pt_model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "model_save_path = os.path.join(pt_model_dir, f\"best_model_{timestamp}.pth\")\n",
    "pt_model_save_path = os.path.join(pt_model_dir, f\"best_model_{timestamp}.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650a99a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce3111c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    model.train()\n",
    "    all_train_mro_preds = []\n",
    "    all_train_mro_targets = []\n",
    "    for train_inputs, train_targets, train_lengths in train_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_inputs = train_inputs.to(device)\n",
    "        train_targets = train_targets.to(device)\n",
    "        # train_lengths = train_lengths.to(device)\n",
    "\n",
    "        model_out = model(train_inputs, train_lengths)\n",
    "\n",
    "        loss = criterion(model_out[:, -1, :], train_targets[:, -1, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        mro_pred = torch.sigmoid(model_out[:, -1, :])\n",
    "        mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "        mro_targets = train_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "        all_train_mro_preds.extend(mro_preds)\n",
    "        all_train_mro_targets.extend(mro_targets)\n",
    "\n",
    "    average_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss: {average_loss}\")\n",
    "\n",
    "    train_f1 = f1_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training F1 Score: {train_f1}\")\n",
    "    train_accuracy = accuracy_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    train_recall = recall_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Recall: {train_recall}\")\n",
    "    train_precision = precision_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Precision: {train_precision}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_mro_preds = []\n",
    "    all_val_mro_targets = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets, test_lengths in test_dataloader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_targets = val_targets.to(device)\n",
    "            model_out = model(val_inputs, test_lengths)\n",
    "            loss = criterion(model_out[:, -1, :], val_targets[:, -1, :])\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            mro_pred = torch.sigmoid(model_out[:, -1, :])\n",
    "            mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "            mro_targets = val_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "            all_val_mro_preds.extend(mro_preds)\n",
    "            all_val_mro_targets.extend(mro_targets)\n",
    "\n",
    "        average_val_loss = val_loss / len(test_dataloader)\n",
    "        print(f\"Validation Loss: {average_val_loss}\")\n",
    "        val_f1 = f1_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation F1 Score: {val_f1}\")\n",
    "        val_accuracy = accuracy_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        val_recall = recall_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Recall: {val_recall}\")\n",
    "        val_precision = precision_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Precision: {val_precision}\")\n",
    "\n",
    "    log_entry = {\n",
    "        \"Epoch\": epoch,\n",
    "        \"Train Loss\": average_loss,\n",
    "        \"Train F1\": train_f1,\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Train Recall\": train_recall,\n",
    "        \"Train Precision\": train_precision,\n",
    "        \"Val Loss\": average_val_loss,\n",
    "        \"Val F1\": val_f1,\n",
    "        \"Val Accuracy\": val_accuracy,\n",
    "        \"Val Recall\": val_recall,\n",
    "        \"Val Precision\": val_precision,\n",
    "    }\n",
    "    entry_df = pd.DataFrame([log_entry])\n",
    "    entry_df.to_csv(log_file_path, mode='a', header=False, index=False)\n",
    "    \n",
    "    if average_val_loss < best_val_loss:\n",
    "        best_val_loss = average_val_loss\n",
    "        counter = 0\n",
    "        # save the model\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        torchscript_model = torch.jit.script(model)\n",
    "        torchscript_model.save(pt_model_save_path)\n",
    "        best_epoch = epoch\n",
    "       \n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            print('Best Epoch is:', best_epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4a76196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17e2df01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
