{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "648daa26",
   "metadata": {},
   "source": [
    "# LSTM Ray Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa55db5",
   "metadata": {},
   "source": [
    "## Data Preprocessing\n",
    "1. target mro selection -- [\"mro\"] or [\"sub_mro1\", \"sub_mro2\", ...]\n",
    "2. add previous mro\n",
    "3. dealing with purchase time\n",
    "1. Standardization\n",
    "    - Continuous Features\n",
    "    - Categorical Features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff72824d",
   "metadata": {},
   "source": [
    "--- "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7447db44",
   "metadata": {},
   "source": [
    "### Target MRO Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5c04b6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_name = \"./Data/mro_daily_clean.csv\"\n",
    "data = pd.read_csv(file_name, index_col=0, engine=\"pyarrow\")\n",
    "\n",
    "# control parameter: target_mro\n",
    "# a list defined by user\n",
    "target_mro: list = [\"mro\"]\n",
    "\n",
    "\n",
    "mro_detail = [\n",
    "    \"battery_dummy\",\n",
    "    \"brake_dummy\",\n",
    "    \"tire_dummy\",\n",
    "    \"lof_dummy\",\n",
    "    \"wiper_dummy\",\n",
    "    \"filter_dummy\",\n",
    "    \"others\",\n",
    "]\n",
    "if target_mro == [\"mro\"]:\n",
    "    data[\"target_mro\"] = data[\"mro\"]\n",
    "elif isinstance(target_mro, list) and all(col in mro_detail for col in target_mro):\n",
    "    data[\"target_mro\"] = data[target_mro].max(axis=1)\n",
    "else:\n",
    "    print(\"Target MRO is defined with error\")\n",
    "    print(\"Use the mro as default mro\")\n",
    "    target_mro = [\"mro\"]\n",
    "    data[\"target_mro\"] = data[\"mro\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "443a43cf",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "449dab18",
   "metadata": {},
   "source": [
    "### Add Previous MRO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfe7d3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control parameter: add_mro_prev\n",
    "add_mro_prev: bool = True\n",
    "\n",
    "\n",
    "if add_mro_prev:\n",
    "    data.sort_values(by=[\"id\", \"yr_nbr\", \"week_nbr\"], inplace=True)\n",
    "    data[\"mro_prev\"] = data.groupby(\"id\")[\"mro\"].shift(1)\n",
    "    mro_prev = [\"mro_prev\"]\n",
    "else:\n",
    "    mro_prev = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51fc6af7",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e684d18",
   "metadata": {},
   "source": [
    "### Dealing with Purchase Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "554e10f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control parameter: add_purchase_time\n",
    "add_purchase_time: bool = True\n",
    "\n",
    "\n",
    "if add_purchase_time:\n",
    "    data[\"purchase_month\"] = data[\"purchase_mth_nbr\"].astype(int)\n",
    "    # devide into 2 bins: 1-6 is the first half, 7-12 is the second half\n",
    "    data[\"purchase_half_year\"] = pd.cut(\n",
    "        data[\"purchase_month\"], bins=[0, 6, 12], labels=[\"first_half\", \"second_half\"]\n",
    "    )\n",
    "\n",
    "    data[\"purchase_time\"] = (\n",
    "        data[\"purchase_yr_nbr\"].astype(int).astype(str)\n",
    "        + \"_\"\n",
    "        + data[\"purchase_half_year\"].astype(str)\n",
    "    )\n",
    "\n",
    "    purchase_time = [\"purchase_time\"]\n",
    "else:\n",
    "    purchase_time = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adabf83a",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c640315e",
   "metadata": {},
   "source": [
    "### Weekly Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "878a39aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "continuous_variable = [\n",
    "    \"hard_braking\",\n",
    "    \"hard_acceleration\",\n",
    "    \"speeding_sum\",\n",
    "    \"day_mileage\",\n",
    "    \"engn_size\",\n",
    "    \"est_hh_incm_prmr_cd\",\n",
    "    \"purchaser_age_at_tm_of_purch\",\n",
    "    \"tavg\",\n",
    "    \"random_avg_traffic\",\n",
    "]\n",
    "\n",
    "category_variable = [\n",
    "    \"gmqualty_model\",\n",
    "    \"umf_xref_finc_gbl_trim\",\n",
    "    \"input_indiv_gndr_prmr_cd\",\n",
    "] + purchase_time\n",
    "\n",
    "driver_navigation = [\n",
    "    \"id\",\n",
    "    \"yr_nbr\",\n",
    "    \"mth_nbr\",\n",
    "    \"week_nbr\",\n",
    "]\n",
    "\n",
    "data = data[\n",
    "    driver_navigation\n",
    "    + continuous_variable\n",
    "    + category_variable\n",
    "    + mro_prev\n",
    "    + [\"target_mro\"]\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78eef111",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from utils import create_train_test_group\n",
    "\n",
    "# control parameter: aggregation function\n",
    "agg_fun = [\"mean\", \"sum\", \"max\", \"min\", \"std\", \"skew\"]\n",
    "\n",
    "\n",
    "agg_rules = {\n",
    "    \"mth_nbr\": \"first\",\n",
    "    \"target_mro\": \"max\",\n",
    "    \"est_hh_incm_prmr_cd\": \"first\",\n",
    "    \"purchaser_age_at_tm_of_purch\": \"first\",\n",
    "    \"input_indiv_gndr_prmr_cd\": \"first\",\n",
    "    \"gmqualty_model\": \"first\",\n",
    "    \"umf_xref_finc_gbl_trim\": \"first\",\n",
    "    \"engn_size\": \"first\",\n",
    "    \"tavg\": agg_fun,\n",
    "    \"random_avg_traffic\": agg_fun,\n",
    "}\n",
    "\n",
    "# control parameter: add_driver_behavior\n",
    "add_driver_behavior = False\n",
    "\n",
    "# \"hard_braking\": agg_fun,\n",
    "# \"hard_acceleration\": agg_fun,\n",
    "# \"speeding_sum\": agg_fun,\n",
    "# \"day_mileage\": agg_fun,\n",
    "if add_driver_behavior:\n",
    "    agg_rules[\"hard_braking\"] = agg_fun\n",
    "    agg_rules[\"hard_acceleration\"] = agg_fun\n",
    "    agg_rules[\"speeding_sum\"] = agg_fun\n",
    "    agg_rules[\"day_mileage\"] = agg_fun\n",
    "if add_mro_prev:\n",
    "    agg_rules[\"mro_prev\"] = \"max\"\n",
    "if add_purchase_time:\n",
    "    agg_rules[\"purchase_time\"] = \"first\"\n",
    "\n",
    "\n",
    "data = data.groupby([\"id\", \"yr_nbr\", \"week_nbr\"]).agg(agg_rules)\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "def flatten_columns(df: pd.DataFrame):\n",
    "    def clean_col(col):\n",
    "        if isinstance(col, tuple):\n",
    "            col_name, agg_func = col\n",
    "            agg_func = agg_func.strip()\n",
    "            if col_name in ([\"target_mro\"] + mro_prev) and agg_func == \"max\":\n",
    "                return col_name\n",
    "            if agg_func in (\"first\", \"\"):\n",
    "                return col_name\n",
    "            return f\"{col_name}_{agg_func}\"\n",
    "        else:\n",
    "            return col\n",
    "\n",
    "    df.columns = [clean_col(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "data = flatten_columns(data)\n",
    "data.fillna(0, inplace=True)\n",
    "data = data.drop([\"yr_nbr\", \"week_nbr\", \"mth_nbr\"], axis=1)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55627a9d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82c3aa0",
   "metadata": {},
   "source": [
    "### Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46fb4cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_need_std = [\n",
    "    item\n",
    "    for item in data.columns.values.tolist()\n",
    "    if item not in ([\"target_mro\"] + mro_prev + [\"id\"] + category_variable)\n",
    "]\n",
    "\n",
    "col_need_encode = category_variable\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[col_need_std] = scaler.fit_transform(data[col_need_std])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_categorical = encoder.fit_transform(data[col_need_encode])\n",
    "\n",
    "category_counts = [len(encoder.categories_[i]) for i, _ in enumerate(col_need_encode)]\n",
    "\n",
    "onehot_feature_names = []\n",
    "for col_idx, col in enumerate(col_need_encode):\n",
    "    num_categories = category_counts[col_idx]\n",
    "    onehot_feature_names.extend([f\"{col}_onehot_{i}\" for i in range(num_categories)])\n",
    "\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_categorical, index=data.index, columns=onehot_feature_names\n",
    ")\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "data = data.drop(columns=col_need_encode)\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7df1fe",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1544966",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control parameter: sample_frac, test_size, valid_size\n",
    "sample_frac = 1.0\n",
    "test_size = 0.1\n",
    "valid_size = 0.1\n",
    "\n",
    "\n",
    "rnn_features = col_need_std + onehot_feature_names + mro_prev\n",
    "rnn_target = [\"target_mro\"]\n",
    "col_rnn_origin = [\"id\"] + rnn_features + rnn_target\n",
    "data_rnn_origin = data[col_rnn_origin].copy()\n",
    "data_rnn_origin = create_train_test_group(\n",
    "    data_rnn_origin,\n",
    "    sample_frac=sample_frac,\n",
    "    test_size=test_size,\n",
    "    valid_size=valid_size,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "data_rnn_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af493ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The RNN input features are:')\n",
    "print(rnn_features)\n",
    "print(\"The RNN target is:\")\n",
    "print(rnn_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0c7abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn_features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39de4f6d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f972d65d",
   "metadata": {},
   "source": [
    "## Model Training\n",
    "### Build the data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cc94cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import mroRnnDataset\n",
    "# ---------------------------------------------------------\n",
    "max_seq_length = 8\n",
    "\n",
    "train_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"train\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "val_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"valid\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "732d9786",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_size = len(rnn_features)\n",
    "output_size = len(rnn_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "478da05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 - data_rnn_origin[\"target_mro\"].eq(1).mean()\n",
    "print(f\"Alpha value for Focal Loss: {alpha}\")\n",
    "gamma = 4\n",
    "print(f\"Gamma value for Focal Loss: {gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a8ba99",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# log file\n",
    "result_csv_file = \"./Out/ray_lstm_no_driver_behavior/ray_train_log.csv\"\n",
    "# if the file exist, delete it\n",
    "os.makedirs(os.path.dirname(result_csv_file), exist_ok=True)\n",
    "\n",
    "result_csv_file = os.path.abspath(result_csv_file)\n",
    "\n",
    "if os.path.exists(result_csv_file):\n",
    "    os.remove(result_csv_file)\n",
    "    print(f\"Deleted existing CSV file: {result_csv_file}\")\n",
    "# ---------------------------------------------------------\n",
    "# model output\n",
    "output_dir = \"./Out/ray_lstm_no_driver_behavior\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "current_time = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "run_dir = os.path.join(output_dir, current_time)\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "run_dir = os.path.abspath(run_dir)\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# early stop control parameter\n",
    "best_val_loss = float(\"inf\")\n",
    "early_stop_patience = 20\n",
    "early_stop_counter = 0\n",
    "best_epoch = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09dc7f1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# control parameter:\n",
    "rnn_type = \"LSTM\"\n",
    "rnn_output_size = 128\n",
    "bidirectional = True\n",
    "num_layers = 2\n",
    "pooling_method = None\n",
    "num_heads = None\n",
    "use_last_hidden = True\n",
    "\n",
    "learning_rate = 0.0005\n",
    "batch_size = 4096\n",
    "\n",
    "num_workers = 4\n",
    "\n",
    "num_epochs = 1000\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# early stop control parameter\n",
    "best_val_loss = float(\"inf\")\n",
    "early_stop_patience = 20\n",
    "early_stop_counter = 0\n",
    "best_epoch = 0\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31d4ffdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import FocalLoss\n",
    "from model import RnnModel\n",
    "from utils import collate_fn\n",
    "\n",
    "import torch\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "import ray.train.torch\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "def train_func():\n",
    "    global best_val_loss, early_stop_counter, best_epoch\n",
    "    # Model, Loss, Optimizer\n",
    "    model = RnnModel(\n",
    "        rnn_type=rnn_type,\n",
    "        input_size=input_feature_size,\n",
    "        rnn_output_size=rnn_output_size,\n",
    "        output_size=output_size,\n",
    "        bidirectional=bidirectional,\n",
    "        num_layers=num_layers,\n",
    "        pooling_method=pooling_method,\n",
    "        num_heads=num_heads,\n",
    "        use_last_hidden=True,\n",
    "    )\n",
    "    # [1] Prepare model.\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    # model.to(\"cuda\")  # This is done by `prepare_model`\n",
    "    criterion = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Data\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # [2] Prepare dataloader.\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "    val_loader = ray.train.torch.prepare_data_loader(val_dataloader)\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_running_loss = 0.0\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        all_train_mro_preds = []\n",
    "        all_train_mro_targets = []\n",
    "        all_train_mro_scores = []\n",
    "        for train_inputs, train_targets, train_lengths in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_inputs = train_inputs[:, :-1, :]\n",
    "            train_targets = train_targets\n",
    "            train_lengths = train_lengths\n",
    "\n",
    "            # model_out = model(train_inputs, train_lengths)\n",
    "            model_out = model(train_inputs, train_lengths)\n",
    "            # the loss using Focal Loss with mean\n",
    "            # which means the average loss of this batch\n",
    "            loss = criterion(model_out, train_targets[:, -1, :])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # running loss equals to single loss * (train length / batch_szie)\n",
    "            train_running_loss += loss.item()\n",
    "\n",
    "            mro_pred = torch.sigmoid(model_out)\n",
    "            mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "            mro_targets = train_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "            all_train_mro_preds.extend(mro_preds)\n",
    "            all_train_mro_targets.extend(mro_targets)\n",
    "            all_train_mro_scores.extend(\n",
    "                torch.sigmoid(model_out).detach().cpu().numpy().flatten()\n",
    "            )\n",
    "\n",
    "        train_average_loss = train_running_loss / (len(train_loader) / batch_size)\n",
    "        train_f1 = f1_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_accuracy = accuracy_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_recall = recall_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_precision = precision_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_auc = roc_auc_score(all_train_mro_targets, all_train_mro_scores)\n",
    "        # ------------------------------------------------\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_val_mro_preds = []\n",
    "        all_val_mro_targets = []\n",
    "        all_val_mro_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_lengths in val_loader:\n",
    "                val_inputs = val_inputs[:, :-1, :]\n",
    "                val_targets = val_targets\n",
    "\n",
    "                model_out = model(val_inputs, val_lengths)\n",
    "                loss = criterion(model_out, val_targets[:, -1, :])\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                mro_pred = torch.sigmoid(model_out)\n",
    "                mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "                mro_targets = val_targets[:, -1, :].cpu().numpy().flatten()\n",
    "                all_val_mro_preds.extend(mro_preds)\n",
    "                all_val_mro_targets.extend(mro_targets)\n",
    "                all_val_mro_scores.extend(\n",
    "                    torch.sigmoid(model_out).cpu().numpy().flatten()\n",
    "                )\n",
    "\n",
    "        val_average_loss = val_running_loss / (len(val_loader) / batch_size)\n",
    "        val_f1 = f1_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_accuracy = accuracy_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_recall = recall_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_precision = precision_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_auc = roc_auc_score(all_val_mro_targets, all_val_mro_scores)\n",
    "        # ------------------------------------------------\n",
    "        # [3] Report metrics and checkpoint.\n",
    "        metrics = {\n",
    "            \"epoch\": epoch,\n",
    "            \"train_average_loss\": train_average_loss,\n",
    "            \"val_average_loss\": val_average_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"train_recall\": train_recall,\n",
    "            \"train_precision\": train_precision,\n",
    "            \"train_auc\": train_auc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"val_auc\": val_auc,\n",
    "        }\n",
    "\n",
    "        # with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "        #     torch.save(\n",
    "        #         model.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "        #     )\n",
    "        #     ray.train.report(\n",
    "        #         metrics,\n",
    "        #         checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "        #     )\n",
    "        ray.train.report(\n",
    "            metrics,\n",
    "            checkpoint=ray.train.Checkpoint.from_directory(run_dir),\n",
    "        )\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "            df = pd.DataFrame([metrics])\n",
    "            # Append to CSV\n",
    "            write_header = not os.path.exists(result_csv_file)\n",
    "            df.to_csv(result_csv_file, mode=\"a\", header=write_header, index=False)\n",
    "\n",
    "            print(metrics)\n",
    "\n",
    "        if val_average_loss < best_val_loss:\n",
    "            best_val_loss = val_average_loss\n",
    "            best_epoch = epoch\n",
    "            early_stop_counter = 0\n",
    "            torch.save(model.state_dict(), os.path.join(run_dir, \"model.pt\"))\n",
    "        else:\n",
    "            early_stop_counter += 1\n",
    "\n",
    "        if early_stop_counter >= early_stop_patience:\n",
    "            print(f\"Early stopping at epoch {epoch} due to no improvement in val loss.\")\n",
    "            break\n",
    "\n",
    "\n",
    "# [4] Configure scaling and resource requirements.\n",
    "scaling_config = ray.train.ScalingConfig(num_workers=4, use_gpu=True)\n",
    "\n",
    "# [5] Launch distributed training job.\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3680b705",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7de0756",
   "metadata": {},
   "source": [
    "# Result Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff60d1bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load benchmark and our model training log data\n",
    "benchmark_log_path = \"./Out/ray_lstm_no_driver_behavior/ray_train_log.csv\"\n",
    "our_model_log_path = \"./Out/ray_lstm/ray_train_log.csv\"\n",
    "\n",
    "benchmark_data = pd.read_csv(benchmark_log_path)\n",
    "our_model_data = pd.read_csv(our_model_log_path)\n",
    "\n",
    "# set the layout\n",
    "fig, axes = plt.subplots(3, 2, figsize=(16, 18))\n",
    "\n",
    "\n",
    "def plot_metric(\n",
    "    ax,\n",
    "    benchmark_data,\n",
    "    our_model_data,\n",
    "    metric_name,\n",
    "    title,\n",
    "    ylabel,\n",
    "    model1_label=\"Benchmark Model\",\n",
    "    model2_label=\"Our Model\",\n",
    "):\n",
    "    ax.plot(\n",
    "        benchmark_data[\"epoch\"],\n",
    "        benchmark_data[f\"train_{metric_name}\"],\n",
    "        marker=\"o\",\n",
    "        label=f\"{model1_label} Train\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        benchmark_data[\"epoch\"],\n",
    "        benchmark_data[f\"val_{metric_name}\"],\n",
    "        marker=\"s\",\n",
    "        label=f\"{model1_label} Val\",\n",
    "    )\n",
    "\n",
    "    ax.plot(\n",
    "        our_model_data[\"epoch\"],\n",
    "        our_model_data[f\"train_{metric_name}\"],\n",
    "        marker=\"^\",\n",
    "        label=f\"{model2_label} Train\",\n",
    "        linestyle=\"--\",\n",
    "    )\n",
    "    ax.plot(\n",
    "        our_model_data[\"epoch\"],\n",
    "        our_model_data[f\"val_{metric_name}\"],\n",
    "        marker=\"v\",\n",
    "        label=f\"{model2_label} Val\",\n",
    "    )\n",
    "\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"Epoch\")\n",
    "    ax.set_ylabel(ylabel)\n",
    "    ax.legend()\n",
    "    ax.grid(True)\n",
    "\n",
    "\n",
    "# Plot 1: Loss\n",
    "plot_metric(\n",
    "    axes[0, 0],\n",
    "    benchmark_data,\n",
    "    our_model_data,\n",
    "    \"average_loss\",\n",
    "    \"Loss vs Epoch\",\n",
    "    \"Average Loss\",\n",
    ")\n",
    "# Plot 2: F1 Score\n",
    "plot_metric(\n",
    "    axes[0, 1], benchmark_data, our_model_data, \"f1\", \"F1 Score vs Epoch\", \"F1 Score\"\n",
    ")\n",
    "# Plot 3: Recall\n",
    "plot_metric(\n",
    "    axes[1, 0], benchmark_data, our_model_data, \"recall\", \"Recall vs Epoch\", \"Recall\"\n",
    ")\n",
    "# Plot 4: Precision\n",
    "plot_metric(\n",
    "    axes[1, 1],\n",
    "    benchmark_data,\n",
    "    our_model_data,\n",
    "    \"precision\",\n",
    "    \"Precision vs Epoch\",\n",
    "    \"Precision\",\n",
    ")\n",
    "# Plot 5: Accuracy\n",
    "plot_metric(\n",
    "    axes[2, 0],\n",
    "    benchmark_data,\n",
    "    our_model_data,\n",
    "    \"accuracy\",\n",
    "    \"Accuracy vs Epoch\",\n",
    "    \"Accuracy\",\n",
    ")\n",
    "# Plot 6: AUC\n",
    "plot_metric(axes[2, 1], benchmark_data, our_model_data, \"auc\", \"AUC vs Epoch\", \"AUC\")\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd536bf3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c5a721",
   "metadata": {},
   "source": [
    "# Summary of Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a00a5634",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "\n",
    "\n",
    "def preprocess_data(\n",
    "    file_name: str,\n",
    "    target_mro: list,\n",
    "    maintain_repair_mro: str,\n",
    "    add_mro_prev: bool,\n",
    "    add_purchase_time: bool,\n",
    "    add_driver_behavior: bool,\n",
    "    agg_weeks: int,\n",
    "    agg_fun: list,\n",
    "):\n",
    "    data = pd.read_csv(file_name, index_col=0, engine=\"pyarrow\")\n",
    "    print(\"Load the dataset\", file_name, \"successfully.\")\n",
    "    # --------------------------------------------------\n",
    "    # target mro selection\n",
    "    mro_detail = [\n",
    "        \"battery_dummy\",\n",
    "        \"brake_dummy\",\n",
    "        \"tire_dummy\",\n",
    "        \"lof_dummy\",\n",
    "        \"wiper_dummy\",\n",
    "        \"filter_dummy\",\n",
    "        \"others\",\n",
    "    ]\n",
    "    if target_mro == [\"mro\"]:\n",
    "        data[\"target_mro\"] = data[\"mro\"]\n",
    "    elif isinstance(target_mro, list) and all(col in mro_detail for col in target_mro):\n",
    "        data[\"target_mro\"] = data[target_mro].max(axis=1)\n",
    "    else:\n",
    "        print(\"Target MRO is defined with error\")\n",
    "        print(\"Use the mro as default mro\")\n",
    "        target_mro = [\"mro\"]\n",
    "        data[\"target_mro\"] = data[\"mro\"]\n",
    "    print(\"The MRO choosen is:\", target_mro)\n",
    "    # --------------------------------------------------\n",
    "    # choose full mro, maintanance or repair\n",
    "    if maintain_repair_mro == \"maintenance\":\n",
    "        data[\"maintenance\"] = data.where(\n",
    "            (data[\"mro\"] == 1) & (data[\"service_days\"] <= 3), 1, 0\n",
    "        )\n",
    "        data[\"target_mro\"] = data[\"maintenance\"]\n",
    "        print(\"Target MRO is maintenance.\")\n",
    "    elif maintain_repair_mro == \"repair\":\n",
    "        data[\"repair\"] = data.where(\n",
    "            (data[\"mro\"] == 1) & (data[\"service_days\"] > 3), 1, 0\n",
    "        )\n",
    "        data[\"target_mro\"] = data[\"repair\"]\n",
    "        print(\"Target MRO is Repair.\")\n",
    "    else:\n",
    "        print(\"No need to know the maintenance or repair.\")\n",
    "        print(\"Use the Target MRO.\")\n",
    "    # --------------------------------------------------\n",
    "    # # select the purchase year\n",
    "    # if purchase_year_select == 2018:\n",
    "    #     data = data[data[\"purchase_yr_nbr\"] == 2018]\n",
    "    # elif purchase_year_select == 2019:\n",
    "    #     data = data[data[\"purchase_yr_nbr\"] == 2019]\n",
    "    # else:\n",
    "    #     print(\"Not select purchase year.\")\n",
    "    # --------------------------------------------------\n",
    "    # add previous mro\n",
    "    if add_mro_prev:\n",
    "        data.sort_values(by=[\"id\", \"yr_nbr\", \"week_nbr\"], inplace=True)\n",
    "        data[\"mro_prev\"] = data.groupby(\"id\")[\"mro\"].shift(1)\n",
    "        mro_prev = [\"mro_prev\"]\n",
    "    else:\n",
    "        mro_prev = []\n",
    "    print(\"Add Previous MRO:\", add_mro_prev)\n",
    "    # --------------------------------------------------\n",
    "    # dealing with purchase time\n",
    "    if add_purchase_time:\n",
    "        data[\"purchase_month\"] = data[\"purchase_mth_nbr\"].astype(int)\n",
    "        # devide into 2 bins: 1-6 is the first half, 7-12 is the second half\n",
    "        data[\"purchase_half_year\"] = pd.cut(\n",
    "            data[\"purchase_month\"],\n",
    "            bins=[0, 6, 12],\n",
    "            labels=[\"first_half\", \"second_half\"],\n",
    "        )\n",
    "\n",
    "        data[\"purchase_time\"] = (\n",
    "            data[\"purchase_yr_nbr\"].astype(int).astype(str)\n",
    "            + \"_\"\n",
    "            + data[\"purchase_half_year\"].astype(str)\n",
    "        )\n",
    "\n",
    "        purchase_time = [\"purchase_time\"]\n",
    "    else:\n",
    "        purchase_time = []\n",
    "    print(\"Add Purchase Time:\", add_purchase_time)\n",
    "    # --------------------------------------------------\n",
    "    # weekly aggregation\n",
    "    continuous_variable = [\n",
    "        \"hard_braking\",\n",
    "        \"hard_acceleration\",\n",
    "        \"speeding_sum\",\n",
    "        \"day_mileage\",\n",
    "        \"engn_size\",\n",
    "        \"est_hh_incm_prmr_cd\",\n",
    "        \"purchaser_age_at_tm_of_purch\",\n",
    "        \"tavg\",\n",
    "        \"random_avg_traffic\",\n",
    "    ]\n",
    "\n",
    "    category_variable = [\n",
    "        \"gmqualty_model\",\n",
    "        \"umf_xref_finc_gbl_trim\",\n",
    "        \"input_indiv_gndr_prmr_cd\",\n",
    "    ] + purchase_time\n",
    "\n",
    "    driver_navigation = [\n",
    "        \"id\",\n",
    "        \"yr_nbr\",\n",
    "        \"mth_nbr\",\n",
    "        \"week_nbr\",\n",
    "    ]\n",
    "\n",
    "    data = data[\n",
    "        driver_navigation\n",
    "        + continuous_variable\n",
    "        + category_variable\n",
    "        + mro_prev\n",
    "        + [\"target_mro\"]\n",
    "    ]\n",
    "\n",
    "    agg_rules = {\n",
    "        # \"mth_nbr\": \"first\",\n",
    "        \"target_mro\": \"max\",\n",
    "        \"est_hh_incm_prmr_cd\": \"first\",\n",
    "        \"purchaser_age_at_tm_of_purch\": \"first\",\n",
    "        \"input_indiv_gndr_prmr_cd\": \"first\",\n",
    "        \"gmqualty_model\": \"first\",\n",
    "        \"umf_xref_finc_gbl_trim\": \"first\",\n",
    "        \"engn_size\": \"first\",\n",
    "        \"tavg\": agg_fun,\n",
    "        \"random_avg_traffic\": agg_fun,\n",
    "    }\n",
    "    # --------------------------------------------------\n",
    "    if add_driver_behavior:\n",
    "        agg_rules[\"hard_braking\"] = agg_fun\n",
    "        agg_rules[\"hard_acceleration\"] = agg_fun\n",
    "        agg_rules[\"speeding_sum\"] = agg_fun\n",
    "        agg_rules[\"day_mileage\"] = agg_fun\n",
    "    print(\"Add behavior of Driver:\", add_driver_behavior)\n",
    "    # --------------------------------------------------\n",
    "    if add_mro_prev:\n",
    "        agg_rules[\"mro_prev\"] = \"max\"\n",
    "    if add_purchase_time:\n",
    "        agg_rules[\"purchase_time\"] = \"first\"\n",
    "    # --------------------------------------------------\n",
    "    # week aggregate\n",
    "    data[\"group_week\"] = (data[\"week_nbr\"] - 1) // agg_weeks\n",
    "    print(\"Aggregate the data into\", agg_weeks, \"week.\")\n",
    "    # data = data.groupby([\"id\", \"yr_nbr\", \"week_nbr\"]).agg(agg_rules)\n",
    "    data = data.groupby([\"id\", \"yr_nbr\", \"group_week\"]).agg(agg_rules)\n",
    "    # --------------------------------------------------\n",
    "    data.reset_index(inplace=True)\n",
    "\n",
    "    def flatten_columns(df: pd.DataFrame):\n",
    "        def clean_col(col):\n",
    "            if isinstance(col, tuple):\n",
    "                col_name, agg_func = col\n",
    "                agg_func = agg_func.strip()\n",
    "                if col_name in ([\"target_mro\"] + mro_prev) and agg_func == \"max\":\n",
    "                    return col_name\n",
    "                if agg_func in (\"first\", \"\"):\n",
    "                    return col_name\n",
    "                return f\"{col_name}_{agg_func}\"\n",
    "            else:\n",
    "                return col\n",
    "\n",
    "        df.columns = [clean_col(col) for col in df.columns]\n",
    "        return df\n",
    "\n",
    "    data = flatten_columns(data)\n",
    "    data.fillna(0, inplace=True)\n",
    "    # data = data.drop([\"yr_nbr\", \"week_nbr\", \"mth_nbr\", \"group_week\"], axis=1)\n",
    "    # data = data.drop([\"yr_nbr\", \"mth_nbr\", \"group_week\"], axis=1)\n",
    "    data = data.drop([\"yr_nbr\", \"group_week\"], axis=1)\n",
    "    # --------------------------------------------------\n",
    "    # Standardization\n",
    "    col_need_std = [\n",
    "        item\n",
    "        for item in data.columns.values.tolist()\n",
    "        if item not in ([\"target_mro\"] + mro_prev + [\"id\"] + category_variable)\n",
    "    ]\n",
    "\n",
    "    col_need_encode = category_variable\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    data[col_need_std] = scaler.fit_transform(data[col_need_std])\n",
    "\n",
    "    encoder = OneHotEncoder(sparse_output=False)\n",
    "    encoded_categorical = encoder.fit_transform(data[col_need_encode])\n",
    "\n",
    "    category_counts = [\n",
    "        len(encoder.categories_[i]) for i, _ in enumerate(col_need_encode)\n",
    "    ]\n",
    "\n",
    "    onehot_feature_names = []\n",
    "    for col_idx, col in enumerate(col_need_encode):\n",
    "        num_categories = category_counts[col_idx]\n",
    "        onehot_feature_names.extend(\n",
    "            [f\"{col}_onehot_{i}\" for i in range(num_categories)]\n",
    "        )\n",
    "\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_categorical, index=data.index, columns=onehot_feature_names\n",
    "    )\n",
    "    data = pd.concat([data, encoded_df], axis=1)\n",
    "    data = data.drop(columns=col_need_encode)\n",
    "    print(\"Finish the process of data standardization\")\n",
    "\n",
    "    rnn_features = col_need_std + onehot_feature_names + mro_prev\n",
    "    print(\"The RNN features are:\", rnn_features)\n",
    "\n",
    "    rnn_target = [\"target_mro\"]\n",
    "    print(\"The RNN target is:\", rnn_target)\n",
    "\n",
    "    return {\n",
    "        \"data\": data,\n",
    "        \"rnn_features\": rnn_features,\n",
    "        \"rnn_target\": rnn_target,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d6aaf89",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data = preprocess_data(\n",
    "    file_name=\"./Data/mro_daily_clean.csv\",\n",
    "    target_mro=[\"mro\"],\n",
    "    maintain_repair_mro=\"full\",\n",
    "    add_mro_prev=True,\n",
    "    add_purchase_time=True,\n",
    "    add_driver_behavior=True,\n",
    "    agg_weeks=1,\n",
    "    agg_fun=[\"mean\", \"sum\", \"max\", \"min\", \"std\", \"skew\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b240014b",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data[\"data\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ec899a",
   "metadata": {},
   "outputs": [],
   "source": [
    "prep_data[\"rnn_features\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89796bb",
   "metadata": {},
   "source": [
    "* 1-week 3,972,103 rows x 81 cols\n",
    "* 2-week 2,104,340 rows x 81 cols\n",
    "* 8-week 664,004 rows x 81 cols\n",
    "    * 415,819 rows x 66 cols (2018)\n",
    "    * 248,185 rows x 77 cols (2019)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5632df",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = prep_data[\"data\"]\n",
    "rnn_features = prep_data[\"rnn_features\"]\n",
    "rnn_target = prep_data[\"rnn_target\"]\n",
    "col_rnn_origin = [\"id\"] + rnn_features + rnn_target\n",
    "data_rnn_origin = data[col_rnn_origin].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78b59a58",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_rnn_origin"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
