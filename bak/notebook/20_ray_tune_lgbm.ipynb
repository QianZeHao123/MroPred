{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d823dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from model import preprocess_data_lgbm as preprocess_data\n",
    "import os\n",
    "import pandas as pd\n",
    "from utils import create_train_test_group\n",
    "import lightgbm as lgb\n",
    "\n",
    "\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.tune.integration.lightgbm import TuneReportCheckpointCallback\n",
    "\n",
    "\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cafbf21a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nEnd of the Parameter Config\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "The following part is the important parameter for the LightGBM training\n",
    "\"\"\"\n",
    "csv_file_name = \"./Data/mro_daily_clean.csv\"\n",
    "target_mro = [\"mro\"]\n",
    "\n",
    "maintain_repair_mro = \"full\"\n",
    "\n",
    "add_mro_prev = True\n",
    "add_purchase_time = True\n",
    "add_driver_behavior = True\n",
    "agg_weeks = 1\n",
    "agg_fun = [\"mean\", \"sum\", \"max\", \"min\", \"std\", \"skew\"]\n",
    "# time window could be 4, 8, 12\n",
    "time_window = 8\n",
    "\n",
    "# ------------------------------------------\n",
    "# LightGBM Parameters\n",
    "metric: list = [\"binary_logloss\", \"binary_error\", \"auc\", \"average_precision\"]\n",
    "learning_rate: float = 0.05\n",
    "num_leaves: int = 64\n",
    "max_depth: int = 8\n",
    "is_unbalance: bool = True\n",
    "# boosting could be \"gbdt\", \"rf\" (random forest) and \"dart\"\n",
    "boosting: str = \"gbdt\"\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# data record folder\n",
    "data_lgbm_file_name = f\"data_lgbm_db{int(add_driver_behavior)}_mp{int(add_mro_prev)}_pt{int(add_purchase_time)}_aw{agg_weeks}_tw{time_window}.gzip\"\n",
    "data_lgbm_path = os.path.join(\"./Data\", data_lgbm_file_name)\n",
    "data_lgbm_path = os.path.abspath(data_lgbm_path)\n",
    "\n",
    "# ------------------------------------------\n",
    "# model record folder\n",
    "model_name = f\"model_lgbm_{boosting}_db{int(add_driver_behavior)}_mp{int(add_mro_prev)}_pt{int(add_purchase_time)}_aw{agg_weeks}_tw{time_window}.txt\"\n",
    "model_output_dir = \"./output/lgbm\"\n",
    "os.makedirs(model_output_dir, exist_ok=True)\n",
    "model_path = os.path.join(model_output_dir, model_name)\n",
    "\n",
    "\n",
    "# ------------------------------------------\n",
    "# train control and scaling control parameters\n",
    "num_workers = 4\n",
    "num_boost_round = 1000\n",
    "early_stopping_round = 10\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "End of the Parameter Config\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d511ccc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data file /home/user14/Cyber/MroPred/Data/data_lgbm_db1_mp1_pt1_aw1_tw8.gzip exists.\n"
     ]
    }
   ],
   "source": [
    "if os.path.isfile(data_lgbm_path):\n",
    "    print(f\"Data file {data_lgbm_path} exists.\")\n",
    "    data_lgbm = pd.read_parquet(data_lgbm_path)\n",
    "else:\n",
    "    print(f\"{data_lgbm_path} does not exist.\")\n",
    "    # control parameter: data preparation\n",
    "    data = preprocess_data(\n",
    "        file_name=csv_file_name,\n",
    "        target_mro=target_mro,\n",
    "        maintain_repair_mro=maintain_repair_mro,\n",
    "        add_mro_prev=add_mro_prev,\n",
    "        add_purchase_time=add_purchase_time,\n",
    "        add_driver_behavior=add_driver_behavior,\n",
    "        agg_weeks=agg_weeks,\n",
    "        agg_fun=agg_fun,\n",
    "        time_window=time_window,\n",
    "    )\n",
    "\n",
    "    data_lgbm = create_train_test_group(\n",
    "        data=data,\n",
    "        sample_frac=1.0,\n",
    "        test_size=0.1,\n",
    "        valid_size=0.1,\n",
    "        random_state=42,\n",
    "    )\n",
    "\n",
    "    data_lgbm.to_parquet(data_lgbm_path, compression=\"gzip\", engine=\"pyarrow\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "09fcdef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(data: pd.DataFrame):\n",
    "    \"\"\"Load and split the dataset into train, validation, and test sets.\"\"\"\n",
    "\n",
    "    train_dataset = data[data[\"group\"] == \"train\"]\n",
    "    valid_dataset = data[data[\"group\"] == \"valid\"]\n",
    "    test_dataset = data[data[\"group\"] == \"test\"]\n",
    "\n",
    "    train_dataset = train_dataset.drop([\"group\", \"id\"], axis=1)\n",
    "    valid_dataset = valid_dataset.drop([\"group\", \"id\"], axis=1)\n",
    "    test_dataset = test_dataset.drop([\"group\", \"id\"], axis=1)\n",
    "\n",
    "    return train_dataset, valid_dataset, test_dataset\n",
    "\n",
    "\n",
    "train_dataset, valid_dataset, test_dataset = prepare_data(data_lgbm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bdce67",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lgbm(config, train_dataset: pd.DataFrame, valid_dataset: pd.DataFrame):\n",
    "    # train_dataset, valid_dataset, _ = prepare_data(data_lgbm)\n",
    "    # train set\n",
    "    train_set = lgb.Dataset(\n",
    "        train_dataset.drop([\"target_mro\"], axis=1), label=train_dataset[\"target_mro\"]\n",
    "    )\n",
    "    # valid set\n",
    "    valid_X = valid_dataset.drop([\"target_mro\"], axis=1)\n",
    "    valid_y = valid_dataset[\"target_mro\"]\n",
    "    valid_set = lgb.Dataset(valid_X, label=valid_y)\n",
    "\n",
    "    gbm = lgb.train(\n",
    "        config,\n",
    "        train_set,\n",
    "        valid_sets=[valid_set],\n",
    "        valid_names=[\"eval\"],\n",
    "        callbacks=[\n",
    "            TuneReportCheckpointCallback(\n",
    "                {\n",
    "                    \"binary_error\": \"eval-binary_error\",\n",
    "                    \"auc\": \"eval-auc\"\n",
    "                    # \"binary_logloss\": \"eval-binary_logloss\",\n",
    "                }\n",
    "            )\n",
    "        ],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d8a9c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Already exists ./output/lgbm/lgbm_tuning_results, remove it and create a new one.\n"
     ]
    }
   ],
   "source": [
    "tune_result_storage_path = './output/lgbm/lgbm_tuning_results'\n",
    "\n",
    "if os.path.exists(tune_result_storage_path):\n",
    "    shutil.rmtree(tune_result_storage_path)\n",
    "    print(f\"Already exists {tune_result_storage_path}, remove it and create a new one.\")\n",
    "    \n",
    "os.makedirs(tune_result_storage_path, exist_ok=True)\n",
    "tune_result_storage_path = os.path.abspath(tune_result_storage_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "015d9753",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-06-08 19:08:37</td></tr>\n",
       "<tr><td>Running for: </td><td>00:09:52.57        </td></tr>\n",
       "<tr><td>Memory:      </td><td>374.1/1007.5 GiB   </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=20<br>Bracket: Iter 64.000: 0.6205431884702372 | Iter 16.000: 0.620141558834611 | Iter 4.000: 0.6189302396896876 | Iter 1.000: 0.6159675457115146<br>Logical resource usage: 16.0/40 CPUs, 0/10 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name            </th><th>status    </th><th>loc                   </th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  max_depth</th><th style=\"text-align: right;\">  num_leaves</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  binary_error</th><th style=\"text-align: right;\">     auc</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_lgbm_8c17a_00000</td><td>TERMINATED</td><td>144.214.55.187:2735290</td><td style=\"text-align: right;\">    1.59272e-07</td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">         452</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        183.682 </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.617553</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00001</td><td>TERMINATED</td><td>144.214.55.187:2735291</td><td style=\"text-align: right;\">    6.59116e-05</td><td style=\"text-align: right;\">          7</td><td style=\"text-align: right;\">          81</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">         84.82  </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.616567</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00002</td><td>TERMINATED</td><td>144.214.55.187:2735741</td><td style=\"text-align: right;\">    3.92115e-08</td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">         416</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.2963</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.610633</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00003</td><td>TERMINATED</td><td>144.214.55.187:2735905</td><td style=\"text-align: right;\">    1.4909e-05 </td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">         987</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         25.195 </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.61138 </td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00004</td><td>TERMINATED</td><td>144.214.55.187:2736048</td><td style=\"text-align: right;\">    0.0109627  </td><td style=\"text-align: right;\">         16</td><td style=\"text-align: right;\">         815</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         26.62  </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.613278</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00005</td><td>TERMINATED</td><td>144.214.55.187:2736185</td><td style=\"text-align: right;\">    1.92851e-06</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">         432</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         26.7715</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.615617</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00006</td><td>TERMINATED</td><td>144.214.55.187:2736294</td><td style=\"text-align: right;\">    5.64033e-06</td><td style=\"text-align: right;\">         11</td><td style=\"text-align: right;\">         245</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        138.529 </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.618459</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00007</td><td>TERMINATED</td><td>144.214.55.187:2736436</td><td style=\"text-align: right;\">    6.1235e-06 </td><td style=\"text-align: right;\">          6</td><td style=\"text-align: right;\">         204</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         22.4241</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.610633</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00008</td><td>TERMINATED</td><td>144.214.55.187:2736588</td><td style=\"text-align: right;\">    4.66871e-06</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">         880</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.4354</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.596727</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00009</td><td>TERMINATED</td><td>144.214.55.187:2736744</td><td style=\"text-align: right;\">    0.000688985</td><td style=\"text-align: right;\">         14</td><td style=\"text-align: right;\">         911</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         27.179 </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.611934</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00010</td><td>TERMINATED</td><td>144.214.55.187:2736889</td><td style=\"text-align: right;\">    2.35216e-08</td><td style=\"text-align: right;\">         12</td><td style=\"text-align: right;\">         883</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         29.3801</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.611808</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00011</td><td>TERMINATED</td><td>144.214.55.187:2737034</td><td style=\"text-align: right;\">    3.87085e-05</td><td style=\"text-align: right;\">         18</td><td style=\"text-align: right;\">         197</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        158.566 </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.621098</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00012</td><td>TERMINATED</td><td>144.214.55.187:2737143</td><td style=\"text-align: right;\">    0.00470798 </td><td style=\"text-align: right;\">         18</td><td style=\"text-align: right;\">         577</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         25.6692</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.615361</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00013</td><td>TERMINATED</td><td>144.214.55.187:2737312</td><td style=\"text-align: right;\">    1.68548e-05</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">         755</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         24.1197</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.596727</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00014</td><td>TERMINATED</td><td>144.214.55.187:2737479</td><td style=\"text-align: right;\">    0.0147744  </td><td style=\"text-align: right;\">         19</td><td style=\"text-align: right;\">         213</td><td style=\"text-align: right;\">   100</td><td style=\"text-align: right;\">        144.671 </td><td style=\"text-align: right;\">     0.0869729</td><td style=\"text-align: right;\">0.636893</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00015</td><td>TERMINATED</td><td>144.214.55.187:2737711</td><td style=\"text-align: right;\">    4.32254e-05</td><td style=\"text-align: right;\">          4</td><td style=\"text-align: right;\">         163</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         27.461 </td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.596727</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00016</td><td>TERMINATED</td><td>144.214.55.187:2737853</td><td style=\"text-align: right;\">    2.85374e-08</td><td style=\"text-align: right;\">         10</td><td style=\"text-align: right;\">         752</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         24.5645</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.614232</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00017</td><td>TERMINATED</td><td>144.214.55.187:2737985</td><td style=\"text-align: right;\">    1.34674e-05</td><td style=\"text-align: right;\">         17</td><td style=\"text-align: right;\">         859</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.2415</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.61339 </td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00018</td><td>TERMINATED</td><td>144.214.55.187:2738094</td><td style=\"text-align: right;\">    1.63575e-06</td><td style=\"text-align: right;\">         17</td><td style=\"text-align: right;\">         711</td><td style=\"text-align: right;\">     1</td><td style=\"text-align: right;\">         23.5219</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.614429</td></tr>\n",
       "<tr><td>train_lgbm_8c17a_00019</td><td>TERMINATED</td><td>144.214.55.187:2738246</td><td style=\"text-align: right;\">    3.3762e-08 </td><td style=\"text-align: right;\">         18</td><td style=\"text-align: right;\">         399</td><td style=\"text-align: right;\">     4</td><td style=\"text-align: right;\">         27.3972</td><td style=\"text-align: right;\">     0.0477882</td><td style=\"text-align: right;\">0.617207</td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.753888 seconds.\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/lightgbm/_lightgbm_utils.py:145: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/user14/Cyber/MroPred/output/lgbm/lgbm_tuning_results/lgbm_tuning_experiment/train_lgbm_8c17a_00001_1_learning_rate=0.0001,max_depth=7,num_leaves=81_2025-06-08_18-58-45/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.834430 seconds.\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2735291)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.538750 seconds.\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2735741)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.566027 seconds.\n",
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2735905)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.969611 seconds.\n",
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2736048)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/lightgbm/_lightgbm_utils.py:145: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2735290)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/user14/Cyber/MroPred/output/lgbm/lgbm_tuning_results/lgbm_tuning_experiment/train_lgbm_8c17a_00000_0_learning_rate=0.0000,max_depth=14,num_leaves=452_2025-06-08_18-58-45/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.484257 seconds.\n",
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2736185)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.305977 seconds.\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.404782 seconds.\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2736436)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.694520 seconds.\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2736588)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.869453 seconds.\n",
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2736744)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.916674 seconds.\n",
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2736889)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/lightgbm/_lightgbm_utils.py:145: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/user14/Cyber/MroPred/output/lgbm/lgbm_tuning_results/lgbm_tuning_experiment/train_lgbm_8c17a_00006_6_learning_rate=0.0000,max_depth=11,num_leaves=245_2025-06-08_18-58-45/checkpoint_000000)\n",
      "\u001b[36m(train_lgbm pid=2736294)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.974849 seconds.\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.225610 seconds.\n",
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2737143)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.830783 seconds.\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2737312)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.356479 seconds.\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/lightgbm/_lightgbm_utils.py:145: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/user14/Cyber/MroPred/output/lgbm/lgbm_tuning_results/lgbm_tuning_experiment/train_lgbm_8c17a_00011_11_learning_rate=0.0000,max_depth=18,num_leaves=197_2025-06-08_18-58-45/checkpoint_000000)\n",
      "\u001b[36m(train_lgbm pid=2737034)\u001b[0m   _log_deprecation_warning(\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 3.096398 seconds.\n",
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737711)\u001b[0m [LightGBM] [Warning] No further splits with positive gain, best gain: -inf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/context.py:131: RayDeprecationWarning: `ray.train.get_context()` should be switched to `ray.tune.get_context()` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/lightgbm/_lightgbm_utils.py:145: RayDeprecationWarning: This API is deprecated and may be removed in future Ray releases. You could suppress this warning by setting env variable PYTHONWARNINGS=\"ignore::DeprecationWarning\"\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m `get_world_rank` is deprecated for Ray Tune because there is no concept of worker ranks for Ray Tune, so these methods only make sense to use in the context of a Ray Train worker. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m   if ray.train.get_context().get_world_rank() in (0, None):\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/tune/trainable/trainable_fn_utils.py:41: RayDeprecationWarning: The `Checkpoint` class should be imported from `ray.tune` when passing it to `ray.tune.report` in a Tune function. Please update your imports. See this issue for more context and migration options: https://github.com/ray-project/ray/issues/49454. Disable these warnings by setting the environment variable: RAY_TRAIN_ENABLE_V2_MIGRATION_WARNINGS=0\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2737479)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/home/user14/Cyber/MroPred/output/lgbm/lgbm_tuning_results/lgbm_tuning_experiment/train_lgbm_8c17a_00014_14_learning_rate=0.0148,max_depth=19,num_leaves=213_2025-06-08_18-58-45/checkpoint_000000)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.341887 seconds.\n",
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2737853)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.432897 seconds.\n",
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2737985)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.347181 seconds.\n",
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "\u001b[36m(train_lgbm pid=2738094)\u001b[0m   _log_deprecation_warning(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m [LightGBM] [Info] Number of positive: 150915, number of negative: 3019730\n",
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m [LightGBM] [Info] Auto-choosing col-wise multi-threading, the overhead of testing was 2.292767 seconds.\n",
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m You can set `force_col_wise=true` to remove the overhead.\n",
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m [LightGBM] [Info] Total Bins 56850\n",
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m [LightGBM] [Info] Number of data points in the train set: 3170645, number of used features: 296\n",
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m [LightGBM] [Info] [binary:BoostFromScore]: pavg=0.047598 -> initscore=-2.996206\n",
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m [LightGBM] [Info] Start training from score -2.996206\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m   _log_deprecation_warning(\n",
      "\u001b[36m(train_lgbm pid=2738246)\u001b[0m /home/user14/data/anaconda3/envs/mro/lib/python3.11/site-packages/ray/train/_internal/session.py:772: RayDeprecationWarning: `ray.train.report` should be switched to `ray.tune.report` when running in a function passed to Ray Tune. This will be an error in the future. See this issue for more context: https://github.com/ray-project/ray/issues/49454\n",
      "2025-06-08 19:08:37,757\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/home/user14/Cyber/MroPred/output/lgbm/lgbm_tuning_results/lgbm_tuning_experiment' in 0.0129s.\n",
      "2025-06-08 19:08:37,773\tINFO tune.py:1041 -- Total run time: 592.61 seconds (592.56 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    config = {\n",
    "        \"objective\": \"binary\",\n",
    "        \"metric\": [\"binary_logloss\", \"binary_error\", \"auc\", \"average_precision\"],\n",
    "        \"verbose\": 1,\n",
    "        \"is_unbalance\": True,\n",
    "        # \"max_depth\": 8,\n",
    "        \"max_depth\": tune.randint(4, 20),\n",
    "        \"boosting_type\": \"gbdt\",\n",
    "        \"device_type\": \"cpu\",\n",
    "        \"num_leaves\": tune.randint(10, 1000),\n",
    "        \"learning_rate\": tune.loguniform(1e-8, 1e-1),\n",
    "        # \"learning_rate\": 0.05,\n",
    "    }\n",
    "\n",
    "    tuner = tune.Tuner(\n",
    "        # train_lgbm,\n",
    "        # tune.with_parameters(\n",
    "        #     train_lgbm, train_dataset=train_dataset, valid_dataset=valid_dataset\n",
    "        # ),\n",
    "        tune.with_resources(\n",
    "            tune.with_parameters(\n",
    "                train_lgbm, train_dataset=train_dataset, valid_dataset=valid_dataset\n",
    "            ),\n",
    "            {\"cpu\": 16},\n",
    "        ),\n",
    "        tune_config=tune.TuneConfig(\n",
    "            # metric=\"binary_error\",\n",
    "            # mode=\"min\",\n",
    "            metric=\"auc\",\n",
    "            mode=\"max\",\n",
    "            scheduler=ASHAScheduler(),\n",
    "            num_samples=20,\n",
    "        ),\n",
    "        run_config=tune.RunConfig(\n",
    "            name=\"lgbm_tuning_experiment\",\n",
    "            storage_path=tune_result_storage_path,\n",
    "        ),\n",
    "        param_space=config,\n",
    "    )\n",
    "    results = tuner.fit()\n",
    "    # print(f\"Best hyperparameters found were: {results.get_best_result().config}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20b6da8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_path = os.path.join(results.get_best_result().checkpoint.path, \"model.txt\")\n",
    "booster = lgb.Booster(model_file=best_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "94184415",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation on Train Set:\n",
      "Accuracy:  0.91292\n",
      "Precision: 0.13650\n",
      "Recall:    0.15574\n",
      "F1 Score:  0.14549\n",
      "AUC:       0.67276\n",
      "\n",
      "Evaluation on Validation Set:\n",
      "Accuracy:  0.91303\n",
      "Precision: 0.13003\n",
      "Recall:    0.14409\n",
      "F1 Score:  0.13670\n",
      "AUC:       0.63689\n",
      "\n",
      "Evaluation on Test Set:\n",
      "Accuracy:  0.91381\n",
      "Precision: 0.13321\n",
      "Recall:    0.14744\n",
      "F1 Score:  0.13996\n",
      "AUC:       0.63668\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    accuracy_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "def get_X_y(df):\n",
    "    X = df.drop(\"target_mro\", axis=1)\n",
    "    y = df[\"target_mro\"]\n",
    "    return X, y\n",
    "\n",
    "\n",
    "X_train, y_train = get_X_y(train_dataset)\n",
    "X_valid, y_valid = get_X_y(valid_dataset)\n",
    "X_test, y_test = get_X_y(test_dataset)\n",
    "\n",
    "\n",
    "def predict_and_eval(booster, X, y_true: pd.DataFrame, dataset_name=\"dataset\"):\n",
    "    y_prob = booster.predict(X)\n",
    "    y_pred = (y_prob >= 0.5).astype(int)\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision = precision_score(y_true, y_pred, zero_division=0)\n",
    "    recall = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    auc = roc_auc_score(y_true, y_prob)\n",
    "\n",
    "    print(f\"\\nEvaluation on {dataset_name}:\")\n",
    "    print(f\"Accuracy:  {acc:.5f}\")\n",
    "    print(f\"Precision: {precision:.5f}\")\n",
    "    print(f\"Recall:    {recall:.5f}\")\n",
    "    print(f\"F1 Score:  {f1:.5f}\")\n",
    "    print(f\"AUC:       {auc:.5f}\")\n",
    "\n",
    "    result_df = pd.DataFrame(\n",
    "        {\"y_true\": y_true.values, \"y_prob\": y_prob, \"y_pred\": y_pred}\n",
    "    )\n",
    "\n",
    "    # return acc, precision, recall, f1, auc, result_df\n",
    "    return {\n",
    "        \"auc\": auc,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1_score\": f1,\n",
    "        \"result_df\": result_df,\n",
    "    }\n",
    "\n",
    "\n",
    "# predict_and_eval(booster, X_train, y_train, \"Train Set\")\n",
    "# predict_and_eval(booster, X_valid, y_valid, \"Validation Set\")\n",
    "# predict_and_eval(booster, X_test, y_test, \"Test Set\")\n",
    "train_results = predict_and_eval(booster, X_train, y_train, \"Train Set\")\n",
    "valid_results = predict_and_eval(booster, X_valid, y_valid, \"Validation Set\")\n",
    "test_results = predict_and_eval(booster, X_test, y_test, \"Test Set\")\n",
    "\n",
    "results_df = pd.DataFrame(\n",
    "    {\n",
    "        \"Best Model Path\": best_model_path,\n",
    "        \"Train Accuracy\": [train_results[\"accuracy\"]],\n",
    "        \"Train Precision\": [train_results[\"precision\"]],\n",
    "        \"Train Recall\": [train_results[\"recall\"]],\n",
    "        \"Train F1 Score\": [train_results[\"f1_score\"]],\n",
    "        \"Train AUC\": [train_results[\"auc\"]],\n",
    "        \"Validation Accuracy\": [valid_results[\"accuracy\"]],\n",
    "        \"Validation Precision\": [valid_results[\"precision\"]],\n",
    "        \"Validation Recall\": [valid_results[\"recall\"]],\n",
    "        \"Validation F1 Score\": [valid_results[\"f1_score\"]],\n",
    "        \"Validation AUC\": [valid_results[\"auc\"]],\n",
    "        \"Test Accuracy\": [test_results[\"accuracy\"]],\n",
    "        \"Test Precision\": [test_results[\"precision\"]],\n",
    "        \"Test Recall\": [test_results[\"recall\"]],\n",
    "        \"Test F1 Score\": [test_results[\"f1_score\"]],\n",
    "        \"Test AUC\": [test_results[\"auc\"]],\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d81022c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Best Model Path",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Train Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Train Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Train Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Train F1 Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Train AUC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Validation Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Validation Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Validation Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Validation F1 Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Validation AUC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test Accuracy",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test Precision",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test Recall",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test F1 Score",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Test AUC",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "9aa22124-3a77-4e34-a4ec-4fe1aeae2fd5",
       "rows": [
        [
         "0",
         "/home/user14/Cyber/MroPred/output/lgbm/lgbm_tuning_results/lgbm_tuning_experiment/train_lgbm_8c17a_00014_14_learning_rate=0.0148,max_depth=19,num_leaves=213_2025-06-08_18-58-45/checkpoint_000000/model.txt",
         "0.9129211879601784",
         "0.13650117022574032",
         "0.15574329920816354",
         "0.14548875903733782",
         "0.6727585079438408",
         "0.9130271480296269",
         "0.130029788642489",
         "0.1440922190201729",
         "0.13670030322612717",
         "0.6368931212075717",
         "0.9138107696746018",
         "0.1332136614861354",
         "0.14743556229413918",
         "0.1399642644431209",
         "0.6366753975742914"
        ]
       ],
       "shape": {
        "columns": 16,
        "rows": 1
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Best Model Path</th>\n",
       "      <th>Train Accuracy</th>\n",
       "      <th>Train Precision</th>\n",
       "      <th>Train Recall</th>\n",
       "      <th>Train F1 Score</th>\n",
       "      <th>Train AUC</th>\n",
       "      <th>Validation Accuracy</th>\n",
       "      <th>Validation Precision</th>\n",
       "      <th>Validation Recall</th>\n",
       "      <th>Validation F1 Score</th>\n",
       "      <th>Validation AUC</th>\n",
       "      <th>Test Accuracy</th>\n",
       "      <th>Test Precision</th>\n",
       "      <th>Test Recall</th>\n",
       "      <th>Test F1 Score</th>\n",
       "      <th>Test AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>/home/user14/Cyber/MroPred/output/lgbm/lgbm_tu...</td>\n",
       "      <td>0.912921</td>\n",
       "      <td>0.136501</td>\n",
       "      <td>0.155743</td>\n",
       "      <td>0.145489</td>\n",
       "      <td>0.672759</td>\n",
       "      <td>0.913027</td>\n",
       "      <td>0.13003</td>\n",
       "      <td>0.144092</td>\n",
       "      <td>0.1367</td>\n",
       "      <td>0.636893</td>\n",
       "      <td>0.913811</td>\n",
       "      <td>0.133214</td>\n",
       "      <td>0.147436</td>\n",
       "      <td>0.139964</td>\n",
       "      <td>0.636675</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                     Best Model Path  Train Accuracy  \\\n",
       "0  /home/user14/Cyber/MroPred/output/lgbm/lgbm_tu...        0.912921   \n",
       "\n",
       "   Train Precision  Train Recall  Train F1 Score  Train AUC  \\\n",
       "0         0.136501      0.155743        0.145489   0.672759   \n",
       "\n",
       "   Validation Accuracy  Validation Precision  Validation Recall  \\\n",
       "0             0.913027               0.13003           0.144092   \n",
       "\n",
       "   Validation F1 Score  Validation AUC  Test Accuracy  Test Precision  \\\n",
       "0               0.1367        0.636893       0.913811        0.133214   \n",
       "\n",
       "   Test Recall  Test F1 Score  Test AUC  \n",
       "0     0.147436       0.139964  0.636675  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8fde91",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
