{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "226a6079",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from utils import create_train_test_group\n",
    "from utils import collate_fn\n",
    "\n",
    "from model import FocalLoss\n",
    "from model import RnnModel\n",
    "from model import mroRnnDataset\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3,4,5,6,7,8\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b157ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# data preprocessing control parameter\n",
    "sample_frac = 1\n",
    "test_size = 0.1\n",
    "valid_size = 0.1\n",
    "max_seq_length = 8\n",
    "batch_size = 4096\n",
    "num_workers = 16\n",
    "\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_val_auc = 0.0\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "best_epoch = float(\"inf\")\n",
    "counter = 0\n",
    "patience = 20\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Parameters for RNN Model\n",
    "rnn_type = \"LSTM\"\n",
    "rnn_output_size = 128\n",
    "bidirectional = True\n",
    "# pooling_method value =  None, 'max', 'avg', 'attention', 'multihead_attention'\n",
    "num_layers = 2\n",
    "pooling_method = None\n",
    "num_heads = None\n",
    "use_last_hidden = True\n",
    "agg_fun = [\"mean\", \"sum\", \"max\", \"min\", \"std\", \"skew\"]\n",
    "\n",
    "\n",
    "file_name = \"./Data/mro_daily_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a0fb64d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "data = pd.read_csv(file_name, index_col=0, engine=\"pyarrow\")\n",
    "\n",
    "\n",
    "continuous_variable = [\n",
    "    \"hard_braking\",\n",
    "    \"hard_acceleration\",\n",
    "    \"speeding_sum\",\n",
    "    \"day_mileage\",\n",
    "    \"engn_size\",\n",
    "    \"est_hh_incm_prmr_cd\",\n",
    "    \"purchaser_age_at_tm_of_purch\",\n",
    "    \"tavg\",\n",
    "    \"random_avg_traffic\",\n",
    "]\n",
    "\n",
    "category_variable = [\n",
    "    \"gmqualty_model\",\n",
    "    \"umf_xref_finc_gbl_trim\",\n",
    "    \"input_indiv_gndr_prmr_cd\",\n",
    "]\n",
    "\n",
    "driver_navigation = [\n",
    "    \"id\",\n",
    "    \"yr_nbr\",\n",
    "    \"mth_nbr\",\n",
    "    \"week_nbr\",\n",
    "]\n",
    "\n",
    "mro = [\"mro\"]\n",
    "\n",
    "\n",
    "data = data[driver_navigation + continuous_variable + category_variable + mro]\n",
    "\n",
    "\n",
    "data = data.groupby([\"id\", \"yr_nbr\", \"week_nbr\"]).agg(\n",
    "    {\n",
    "        \"mth_nbr\": \"first\",\n",
    "        \"mro\": \"max\",\n",
    "        \"hard_braking\": agg_fun,\n",
    "        \"hard_acceleration\": agg_fun,\n",
    "        \"speeding_sum\": agg_fun,\n",
    "        \"day_mileage\": agg_fun,\n",
    "        \"est_hh_incm_prmr_cd\": \"first\",\n",
    "        \"purchaser_age_at_tm_of_purch\": \"first\",\n",
    "        \"input_indiv_gndr_prmr_cd\": \"first\",\n",
    "        \"gmqualty_model\": \"first\",\n",
    "        \"umf_xref_finc_gbl_trim\": \"first\",\n",
    "        \"engn_size\": \"first\",\n",
    "        \"tavg\": agg_fun,\n",
    "        \"random_avg_traffic\": agg_fun,\n",
    "    }\n",
    ")\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "def flatten_columns(df: pd.DataFrame):\n",
    "    def clean_col(col):\n",
    "        if isinstance(col, tuple):\n",
    "            col_name, agg_func = col\n",
    "            agg_func = agg_func.strip()\n",
    "            if col_name in mro and agg_func == \"max\":\n",
    "                return \"mro\"\n",
    "            if agg_func in (\"first\", \"\"):\n",
    "                return col_name\n",
    "            return f\"{col_name}_{agg_func}\"\n",
    "        else:\n",
    "            return col\n",
    "\n",
    "    df.columns = [clean_col(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "data = flatten_columns(data)\n",
    "data.fillna(0, inplace=True)\n",
    "data = data.drop([\"yr_nbr\", \"week_nbr\", \"mth_nbr\"], axis=1)\n",
    "\n",
    "\n",
    "col_need_std = [\n",
    "    item\n",
    "    for item in data.columns.values.tolist()\n",
    "    if item not in (mro + [\"id\"] + category_variable)\n",
    "]\n",
    "\n",
    "col_need_encode = category_variable\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[col_need_std] = scaler.fit_transform(data[col_need_std])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_categorical = encoder.fit_transform(data[col_need_encode])\n",
    "\n",
    "category_counts = [len(encoder.categories_[i]) for i, _ in enumerate(col_need_encode)]\n",
    "\n",
    "onehot_feature_names = []\n",
    "for col_idx, col in enumerate(col_need_encode):\n",
    "    num_categories = category_counts[col_idx]\n",
    "    onehot_feature_names.extend([f\"{col}_onehot_{i}\" for i in range(num_categories)])\n",
    "\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_categorical, index=data.index, columns=onehot_feature_names\n",
    ")\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "rnn_features = col_need_std + onehot_feature_names\n",
    "# col_rnn_origin = [\"id\"] + rnn_features + [\"mro\"]\n",
    "col_rnn_origin = [\"id\"] + rnn_features + mro\n",
    "data_rnn_origin = data[col_rnn_origin].copy()\n",
    "data_rnn_origin = create_train_test_group(\n",
    "    data_rnn_origin,\n",
    "    sample_frac=sample_frac,\n",
    "    test_size=test_size,\n",
    "    valid_size=valid_size,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7207ae9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_target = [\"mro\"]\n",
    "rnn_target = mro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f879028",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "train_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"train\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "val_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"valid\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "test_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"test\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# train_dataloader = DataLoader(\n",
    "#     train_data_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=collate_fn,\n",
    "#     num_workers=num_workers,\n",
    "# )\n",
    "\n",
    "# val_dataloader = DataLoader(\n",
    "#     val_data_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=collate_fn,\n",
    "#     num_workers=num_workers,\n",
    "# )\n",
    "\n",
    "# test_dataloader = DataLoader(\n",
    "#     test_data_set,\n",
    "#     batch_size=batch_size,\n",
    "#     shuffle=False,\n",
    "#     collate_fn=collate_fn,\n",
    "#     num_workers=num_workers,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "324faf26",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_size = len(rnn_features)\n",
    "output_size = len(rnn_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69925ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 - data_rnn_origin[\"mro\"].eq(1).mean()\n",
    "print(f\"Alpha value for Focal Loss: {alpha}\")\n",
    "gamma = 4\n",
    "print(f\"Gamma value for Focal Loss: {gamma}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2f3d658",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ray.train.torch\n",
    "import tempfile\n",
    "import csv\n",
    "\n",
    "def train_func():\n",
    "    # Model, Loss, Optimizer\n",
    "    model = RnnModel(\n",
    "        rnn_type=rnn_type,\n",
    "        input_size=input_feature_size,\n",
    "        rnn_output_size=rnn_output_size,\n",
    "        output_size=output_size,\n",
    "        bidirectional=bidirectional,\n",
    "        num_layers=num_layers,\n",
    "        pooling_method=pooling_method,\n",
    "        num_heads=num_heads,\n",
    "        use_last_hidden=True,\n",
    "    )\n",
    "    # [1] Prepare model.\n",
    "    model = ray.train.torch.prepare_model(model)\n",
    "    # model.to(\"cuda\")  # This is done by `prepare_model`\n",
    "    criterion = FocalLoss(alpha=alpha, gamma=gamma)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "\n",
    "    # Data\n",
    "    train_dataloader = DataLoader(\n",
    "        train_data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "    val_dataloader = DataLoader(\n",
    "        val_data_set,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=False,\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=num_workers,\n",
    "    )\n",
    "\n",
    "    # [2] Prepare dataloader.\n",
    "    train_loader = ray.train.torch.prepare_data_loader(train_dataloader)\n",
    "    val_loader = ray.train.torch.prepare_data_loader(val_dataloader)\n",
    "\n",
    "    # log file\n",
    "    result_csv_file = \"./ray_training_results.csv\"\n",
    "\n",
    "    # Training\n",
    "    for epoch in range(num_epochs):\n",
    "        if ray.train.get_context().get_world_size() > 1:\n",
    "            train_loader.sampler.set_epoch(epoch)\n",
    "\n",
    "        train_running_loss = 0.0\n",
    "        # print(f\"Epoch {epoch}\")\n",
    "        model.train()\n",
    "        all_train_mro_preds = []\n",
    "        all_train_mro_targets = []\n",
    "        all_train_mro_scores = []\n",
    "        for train_inputs, train_targets, train_lengths in train_loader:\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            train_inputs = train_inputs[:, :-1, :]\n",
    "            train_targets = train_targets\n",
    "            train_lengths = train_lengths\n",
    "\n",
    "            # model_out = model(train_inputs, train_lengths)\n",
    "            model_out = model(train_inputs, train_lengths)\n",
    "            # the loss using Focal Loss with mean\n",
    "            # which means the average loss of this batch\n",
    "            loss = criterion(model_out, train_targets[:, -1, :])\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # running loss equals to single loss * (train length / batch_szie)\n",
    "            train_running_loss += loss.item()\n",
    "\n",
    "            mro_pred = torch.sigmoid(model_out)\n",
    "            mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "            mro_targets = train_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "            all_train_mro_preds.extend(mro_preds)\n",
    "            all_train_mro_targets.extend(mro_targets)\n",
    "            all_train_mro_scores.extend(\n",
    "                torch.sigmoid(model_out).detach().cpu().numpy().flatten()\n",
    "            )\n",
    "\n",
    "        train_average_loss = train_running_loss / (len(train_loader) / batch_size)\n",
    "        train_f1 = f1_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_accuracy = accuracy_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_recall = recall_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_precision = precision_score(all_train_mro_targets, all_train_mro_preds)\n",
    "        train_auc = roc_auc_score(all_train_mro_targets, all_train_mro_scores)\n",
    "        # ------------------------------------------------\n",
    "        model.eval()\n",
    "        val_running_loss = 0.0\n",
    "        all_val_mro_preds = []\n",
    "        all_val_mro_targets = []\n",
    "        all_val_mro_scores = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets, val_lengths in val_loader:\n",
    "                val_inputs = val_inputs[:, :-1, :]\n",
    "                val_targets = val_targets\n",
    "\n",
    "                model_out = model(val_inputs, val_lengths)\n",
    "                loss = criterion(model_out, val_targets[:, -1, :])\n",
    "                val_running_loss += loss.item()\n",
    "\n",
    "                mro_pred = torch.sigmoid(model_out)\n",
    "                mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "                mro_targets = val_targets[:, -1, :].cpu().numpy().flatten()\n",
    "                all_val_mro_preds.extend(mro_preds)\n",
    "                all_val_mro_targets.extend(mro_targets)\n",
    "                all_val_mro_scores.extend(\n",
    "                    torch.sigmoid(model_out).cpu().numpy().flatten()\n",
    "                )\n",
    "\n",
    "        val_average_loss = val_running_loss / len(val_loader)\n",
    "        val_f1 = f1_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_accuracy = accuracy_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_recall = recall_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_precision = precision_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        val_auc = roc_auc_score(all_val_mro_targets, all_val_mro_scores)\n",
    "        # ------------------------------------------------\n",
    "        # [3] Report metrics and checkpoint.\n",
    "        metrics = {\n",
    "            \"epoch\": epoch,\n",
    "            \"loss\": train_average_loss,\n",
    "            \"val_loss\": val_average_loss,\n",
    "            \"train_f1\": train_f1,\n",
    "            \"train_accuracy\": train_accuracy,\n",
    "            \"train_recall\": train_recall,\n",
    "            \"train_precision\": train_precision,\n",
    "            \"train_auc\": train_auc,\n",
    "            \"val_f1\": val_f1,\n",
    "            \"val_accuracy\": val_accuracy,\n",
    "            \"val_recall\": val_recall,\n",
    "            \"val_precision\": val_precision,\n",
    "            \"val_auc\": val_auc,\n",
    "        }\n",
    "        with tempfile.TemporaryDirectory() as temp_checkpoint_dir:\n",
    "            torch.save(\n",
    "                model.state_dict(), os.path.join(temp_checkpoint_dir, \"model.pt\")\n",
    "            )\n",
    "            ray.train.report(\n",
    "                metrics,\n",
    "                checkpoint=ray.train.Checkpoint.from_directory(temp_checkpoint_dir),\n",
    "            )\n",
    "        if ray.train.get_context().get_world_rank() == 0:\n",
    "\n",
    "            df = pd.DataFrame([metrics])\n",
    "            # Append to CSV\n",
    "            write_header = not os.path.exists(result_csv_file)\n",
    "            df.to_csv(result_csv_file, mode=\"a\", header=write_header, index=False)\n",
    "            print(metrics)\n",
    "\n",
    "\n",
    "# [4] Configure scaling and resource requirements.\n",
    "scaling_config = ray.train.ScalingConfig(num_workers=4, use_gpu=True)\n",
    "\n",
    "# [5] Launch distributed training job.\n",
    "trainer = ray.train.torch.TorchTrainer(\n",
    "    train_func,\n",
    "    scaling_config=scaling_config,\n",
    ")\n",
    "result = trainer.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1a98ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
