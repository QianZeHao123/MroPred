{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "658d617b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of CUDA devices available: 10\n",
      "Using device: NVIDIA GeForce RTX 2080 Ti\n",
      "Using cuda:8 device\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from torch import optim\n",
    "import torch.nn as nn\n",
    "import torch.nn as nn\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "from utils import create_train_test_group\n",
    "\n",
    "# update mroRnnDataset with better performance\n",
    "from utils import mroRnnDataset as mroRnnDataset\n",
    "from utils import collate_fn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available CUDA devices\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_gpus}\")\n",
    "    # Select a specific GPU (e.g., GPU 0)\n",
    "    device = torch.device(\"cuda:8\")  # Use \"cuda:1\" for GPU 1, etc.\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA is not available, using CPU\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6d284d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# data preprocessing control parameter\n",
    "sample_frac = 0.01\n",
    "test_size = 0.1\n",
    "max_seq_length = 50\n",
    "batch_size = 4096\n",
    "num_workers = 16\n",
    "\n",
    "learning_rate = 0.0001\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_val_auc = 0.0\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "best_epoch = float(\"inf\")\n",
    "counter = 0\n",
    "patience = 10\n",
    "\n",
    "time_window_leakage = 5\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Parameters for RNN Model\n",
    "rnn_type = \"GRU\"\n",
    "rnn_output_size = 64\n",
    "bidirectional = True\n",
    "# pooling_method value =  None, 'max', 'avg'\n",
    "# 'attention', 'multihead_attention'\n",
    "num_layers = 3\n",
    "pooling_method = \"attention\"\n",
    "num_heads = 4\n",
    "use_last_hidden = True\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "log_dir = \"./Out\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "log_file_path = os.path.join(log_dir, f\"training_log_{timestamp}_test.csv\")\n",
    "\n",
    "pt_model_dir = os.path.join(log_dir, \"models\")\n",
    "os.makedirs(pt_model_dir, exist_ok=True)\n",
    "\n",
    "\n",
    "model_save_path = os.path.join(pt_model_dir, f\"best_model_{timestamp}.pth\")\n",
    "pt_model_save_path = os.path.join(pt_model_dir, f\"best_model_{timestamp}.pt\")\n",
    "\n",
    "\n",
    "file_name = \"./Data/mro_daily_clean.csv\"\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aeb2533c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(file_name, index_col=0, engine=\"pyarrow\")\n",
    "\n",
    "\n",
    "column_need_std = [\n",
    "    \"hard_braking\",\n",
    "    \"hard_acceleration\",\n",
    "    \"speeding_sum\",\n",
    "    \"day_mileage\",\n",
    "    \"engn_size\",\n",
    "    \"est_hh_incm_prmr_cd\",\n",
    "    \"purchaser_age_at_tm_of_purch\",\n",
    "    \"tavg\",\n",
    "    \"random_avg_traffic\",\n",
    "]\n",
    "\n",
    "column_after_std = [\n",
    "    \"hard_braking_std\",\n",
    "    \"hard_acceleration_std\",\n",
    "    \"speeding_sum_std\",\n",
    "    \"day_mileage_std\",\n",
    "    \"engn_size_std\",\n",
    "    \"est_hh_incm_prmr_cd_std\",\n",
    "    \"purchaser_age_at_tm_of_purch_std\",\n",
    "    \"tavg_std\",\n",
    "    \"random_avg_traffic_std\",\n",
    "]\n",
    "\n",
    "column_need_encode = [\n",
    "    \"gmqualty_model\",\n",
    "    \"umf_xref_finc_gbl_trim\",\n",
    "    \"input_indiv_gndr_prmr_cd\",\n",
    "]\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# standardize data\n",
    "scaler = StandardScaler()\n",
    "data[column_after_std] = scaler.fit_transform(data[column_need_std])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_categorical = encoder.fit_transform(data[column_need_encode])\n",
    "\n",
    "category_counts = [\n",
    "    len(encoder.categories_[i]) for i, _ in enumerate(column_need_encode)\n",
    "]\n",
    "\n",
    "onehot_feature_names = []\n",
    "for col_idx, col in enumerate(column_need_encode):\n",
    "    num_categories = category_counts[col_idx]\n",
    "    onehot_feature_names.extend([f\"{col}_onehot_{i}\" for i in range(num_categories)])\n",
    "\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_categorical, index=data.index, columns=onehot_feature_names\n",
    ")\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "rnn_features = column_after_std + onehot_feature_names\n",
    "# col_rnn_origin = [\"id\"] + rnn_features + [\"mro\"]\n",
    "col_rnn_origin = [\"id\"] + rnn_features + [\"tire_dummy\"]\n",
    "data_rnn_origin = data[col_rnn_origin].copy()\n",
    "data_rnn_origin = create_train_test_group(\n",
    "    data_rnn_origin, sample_frac=sample_frac, test_size=test_size\n",
    ")\n",
    "# rnn_target = [\"mro\"]\n",
    "rnn_target = [\"tire_dummy\"]\n",
    "# ---------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9675bcb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def label_leakage_all_events(\n",
    "#     df: pd.DataFrame,\n",
    "#     id_col=\"id\",\n",
    "#     target_col=\"mro\",\n",
    "#     window_size=5,\n",
    "# ):\n",
    "#     \"\"\"\n",
    "#     For each time series of IDs, find all positions where MRO=1,\n",
    "#     mark the MRO of each time step before each MRO=1 as 1.\n",
    "#     \"\"\"\n",
    "#     df = df.copy()\n",
    "\n",
    "#     def process_group(group: pd.DataFrame):\n",
    "#         # find all the index when MRO=1\n",
    "#         one_indices = group[group[target_col] == 1].index.tolist()\n",
    "#         if not one_indices:  # if no 1 in the seq, return origin seq\n",
    "#             return group\n",
    "\n",
    "#         for first_one_idx in one_indices:\n",
    "#             start_idx = max(group.index.min(), first_one_idx - window_size)\n",
    "#             group.loc[start_idx:first_one_idx, target_col] = 1\n",
    "#         return group\n",
    "\n",
    "#     # Step 1: Remove id_col from dataframe before groupby\n",
    "#     original_id = df[id_col]\n",
    "#     data_to_process = df.drop(columns=[id_col])\n",
    "\n",
    "#     # Step 2: Group and apply function on reduced dataframe\n",
    "#     processed_data = (\n",
    "#         data_to_process.groupby(original_id, group_keys=False)\n",
    "#         .apply(process_group)\n",
    "#         .reset_index(drop=True)\n",
    "#     )\n",
    "\n",
    "#     # Step 3: Re-add the id column back\n",
    "#     processed_data[id_col] = original_id.reset_index(drop=True)\n",
    "\n",
    "#     return processed_data\n",
    "\n",
    "\n",
    "# data_rnn_origin = label_leakage_all_events(\n",
    "#     data_rnn_origin,\n",
    "#     id_col=\"id\",\n",
    "#     target_col=\"mro\",\n",
    "#     window_size=time_window_leakage,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8c7d0e2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "\n",
    "train_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"train\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "test_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"test\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "\n",
    "# Example batch size\n",
    "train_dataloader = DataLoader(\n",
    "    train_data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=True,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73611e01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RnnModel(\n",
      "  (rnn): GRU(44, 64, num_layers=3, batch_first=True, dropout=0.5, bidirectional=True)\n",
      "  (attn): MultiheadAttention(\n",
      "    (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "  )\n",
      "  (fc): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.5, inplace=False)\n",
      "    (3): Linear(in_features=64, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ---------------------------------------------------------\n",
    "\n",
    "\n",
    "class RnnModel(nn.Module):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        rnn_type: str,\n",
    "        input_size,\n",
    "        rnn_output_size,\n",
    "        output_size,\n",
    "        bidirectional: bool = False,\n",
    "        num_layers: int = 3,\n",
    "        pooling_method: str = \"attention\",  # value =  None, 'max', 'avg', 'attention', 'multihead_attention'\n",
    "        num_heads=4,\n",
    "        use_last_hidden: bool = True,\n",
    "    ):\n",
    "        super(RnnModel, self).__init__()\n",
    "        # bidirectional = False\n",
    "        num_directions = 2 if bidirectional else 1\n",
    "        self.embed_dim = rnn_output_size * num_directions\n",
    "        self.pooling_method = pooling_method\n",
    "        self.use_last_hidden = use_last_hidden\n",
    "\n",
    "        if rnn_type == \"LSTM\":\n",
    "            self.rnn = nn.LSTM(\n",
    "                input_size=input_size,\n",
    "                hidden_size=rnn_output_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=0.5,\n",
    "            )\n",
    "        elif rnn_type == \"GRU\":\n",
    "            self.rnn = nn.GRU(\n",
    "                input_size=input_size,\n",
    "                hidden_size=rnn_output_size,\n",
    "                num_layers=num_layers,\n",
    "                batch_first=True,\n",
    "                bidirectional=bidirectional,\n",
    "                dropout=0.5,\n",
    "            )\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported rnn_type: {rnn_type}\")\n",
    "\n",
    "        if pooling_method == \"multihead_attention\":\n",
    "            self.attn = nn.MultiheadAttention(\n",
    "                embed_dim=self.embed_dim, num_heads=num_heads, batch_first=True\n",
    "            )\n",
    "        elif pooling_method == \"attention\":\n",
    "            self.attn = nn.MultiheadAttention(\n",
    "                embed_dim=self.embed_dim, num_heads=1, batch_first=True\n",
    "            )\n",
    "        elif pooling_method in [\"max\", \"avg\"]:\n",
    "            self.pool = getattr(nn, f\"Adaptive{pooling_method.capitalize()}Pool1d\")(1)\n",
    "        elif pooling_method is None:\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported pooling_method: {pooling_method}\")\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(self.embed_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.5),\n",
    "            nn.Linear(64, output_size),\n",
    "        )\n",
    "\n",
    "    def forward(self, x, length: torch.Tensor):\n",
    "        # x (batch_size, seq_len, input_size)\n",
    "        # use pack_padded_sequence and pad_packed_sequence to deal with different length of x input\n",
    "        packed_x = pack_padded_sequence(\n",
    "            x, length, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        packed_out, _ = self.rnn(packed_x)\n",
    "        rnn_out, _ = pad_packed_sequence(packed_out, batch_first=True)\n",
    "        # pooled = self.pool(rnn_out.transpose(1, 2)).squeeze(2)\n",
    "        # attended = self.attention(rnn_out)\n",
    "        if self.pooling_method is None:\n",
    "            if self.use_last_hidden:\n",
    "                idx = (length - 1).to(x.device)\n",
    "                batch_indices = torch.arange(x.size(0), device=x.device)\n",
    "                pooled = rnn_out[batch_indices, idx]  # (B, H)\n",
    "            else:\n",
    "                # use the average to of all outputs\n",
    "                pooled = rnn_out.mean(dim=1)  # (B, H)\n",
    "        elif self.pooling_method in [\"max\", \"avg\"]:\n",
    "            pooled = self.pool(rnn_out.transpose(1, 2)).squeeze(2)\n",
    "        elif self.pooling_method in [\"attention\", \"multihead_attention\"]:\n",
    "            # if self.pooling_method == \"multihead_attention\":\n",
    "            #     attn_out, _ = self.attn(rnn_out, rnn_out, rnn_out)\n",
    "            #     pooled = attn_out.mean(dim=1)\n",
    "            # else:\n",
    "            #     pooled = self.attn(rnn_out)\n",
    "            attn_output, _ = self.attn(rnn_out, rnn_out, rnn_out)\n",
    "            pooled = attn_output.mean(dim=1)\n",
    "        else:\n",
    "            raise ValueError(\"Invalid pooling_method\")\n",
    "\n",
    "        model_out = self.fc(pooled)\n",
    "        return model_out\n",
    "\n",
    "\n",
    "input_feature_size = len(rnn_features)\n",
    "output_size = len(rnn_target)\n",
    "\n",
    "model = RnnModel(\n",
    "    rnn_type=rnn_type,\n",
    "    input_size=input_feature_size,\n",
    "    rnn_output_size=rnn_output_size,\n",
    "    output_size=output_size,\n",
    "    bidirectional=bidirectional,\n",
    "    num_layers=num_layers,\n",
    "    # value =  None, 'max', 'avg', 'attention', 'multihead_attention'\n",
    "    pooling_method=pooling_method,\n",
    "    num_heads=num_heads,\n",
    "    use_last_hidden=True,\n",
    ").to(device)\n",
    "\n",
    "print(model)\n",
    "\n",
    "\n",
    "# criterion = nn.BCELoss()\n",
    "# criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dafbd81f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pos_weight_value = (data_rnn_origin[\"mro\"] == 0).sum() / (\n",
    "#     data_rnn_origin[\"mro\"] == 1\n",
    "# ).sum()\n",
    "# print(f\"pos_weight: {pos_weight_value}\")\n",
    "\n",
    "# criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_value])).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4dccadfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0):\n",
    "        super().__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        bce = nn.functional.binary_cross_entropy_with_logits(\n",
    "            logits, targets, reduction=\"none\"\n",
    "        )\n",
    "        pt = torch.exp(-bce)\n",
    "        focal_weight = (1 - pt) ** self.gamma\n",
    "        alpha_weight = targets * self.alpha + (1 - targets) * (1 - self.alpha)\n",
    "        focal_loss = alpha_weight * focal_weight * bce\n",
    "        return focal_loss.mean()\n",
    "\n",
    "\n",
    "# alpha = 1 - data_rnn_origin[\"mro\"].eq(1).mean()\n",
    "alpha = 1 - data_rnn_origin[\"tire_dummy\"].eq(1).mean()\n",
    "gamma = 8\n",
    "criterion = FocalLoss(alpha=alpha, gamma=gamma).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e490af7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "\n",
    "fieldnames = [\n",
    "    \"Epoch\",\n",
    "    \"Train Loss\",\n",
    "    \"Train F1\",\n",
    "    \"Train Accuracy\",\n",
    "    \"Train Recall\",\n",
    "    \"Train Precision\",\n",
    "    \"Train AUC\",\n",
    "    \"Val Loss\",\n",
    "    \"Val F1\",\n",
    "    \"Val Accuracy\",\n",
    "    \"Val Recall\",\n",
    "    \"Val Precision\",\n",
    "    \"Val AUC\",\n",
    "]\n",
    "\n",
    "log_df = pd.DataFrame(columns=fieldnames)\n",
    "log_df.to_csv(log_file_path, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d7c775ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0\n",
      "Average training loss: 1.0165849051013427e-06\n",
      "Training F1 Score: 0.00034113476041374775\n",
      "Training Accuracy: 0.0019037192470273395\n",
      "Training Recall: 1.0\n",
      "Training Precision: 0.0001705964784012673\n",
      "Training AUC: 0.48859409720441915\n",
      "Validation Loss: 9.098409691432607e-07\n",
      "Validation F1 Score: 0.00036153289949385393\n",
      "Validation Accuracy: 0.0001807991321641656\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0001807991321641656\n",
      "Validation AUC: 0.58681936909785\n",
      "Epoch 1\n",
      "Average training loss: 1.0056285094190282e-06\n",
      "Training F1 Score: 0.00038803417575073834\n",
      "Training Accuracy: 0.15391539701365448\n",
      "Training Recall: 0.9642857142857143\n",
      "Training Precision: 0.00019405613253315124\n",
      "Training AUC: 0.5817513376535683\n",
      "Validation Loss: 8.436945677203767e-07\n",
      "Validation F1 Score: 0.00047770700636942675\n",
      "Validation Accuracy: 0.24341589827035498\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.00023891056781078282\n",
      "Validation AUC: 0.7058268032951578\n",
      "Epoch 2\n",
      "Average training loss: 8.924833201579132e-07\n",
      "Training F1 Score: 0.000410690267667382\n",
      "Training Accuracy: 0.4078581637928413\n",
      "Training Recall: 0.7142857142857143\n",
      "Training Precision: 0.00020540418408322977\n",
      "Training AUC: 0.6319570151967179\n",
      "Validation Loss: 8.209100428757665e-07\n",
      "Validation F1 Score: 0.0006856359273225917\n",
      "Validation Accuracy: 0.6486470198276382\n",
      "Validation Recall: 0.6666666666666666\n",
      "Validation Precision: 0.0003429943405933802\n",
      "Validation AUC: 0.7877637130801687\n",
      "Epoch 3\n",
      "Average training loss: 9.098950943375602e-07\n",
      "Training F1 Score: 0.0004173459406586241\n",
      "Training Accuracy: 0.5338442356232704\n",
      "Training Recall: 0.5714285714285714\n",
      "Training Precision: 0.00020874920088196538\n",
      "Training AUC: 0.5700762529883749\n",
      "Validation Loss: 7.987075264281885e-07\n",
      "Validation F1 Score: 0.0007925500297206261\n",
      "Validation Accuracy: 0.6960766588320376\n",
      "Validation Recall: 0.6666666666666666\n",
      "Validation Precision: 0.0003965107057890563\n",
      "Validation AUC: 0.8255776572232267\n",
      "Epoch 4\n",
      "Average training loss: 8.718679985455177e-07\n",
      "Training F1 Score: 0.0005672222866793002\n",
      "Training Accuracy: 0.5284675972386947\n",
      "Training Recall: 0.7857142857142857\n",
      "Training Precision: 0.0002837135524805592\n",
      "Training AUC: 0.6825750472100245\n",
      "Validation Loss: 7.763295002405357e-07\n",
      "Validation F1 Score: 0.001001001001001001\n",
      "Validation Accuracy: 0.7594166214668836\n",
      "Validation Recall: 0.6666666666666666\n",
      "Validation Precision: 0.0005008765339343851\n",
      "Validation AUC: 0.8410689170182841\n",
      "Epoch 5\n",
      "Average training loss: 8.676949013389698e-07\n",
      "Training F1 Score: 0.0006059093987240261\n",
      "Training Accuracy: 0.6589118997658364\n",
      "Training Recall: 0.6071428571428571\n",
      "Training Precision: 0.0003031059444424634\n",
      "Training AUC: 0.6755967407919814\n",
      "Validation Loss: 7.567413319975458e-07\n",
      "Validation F1 Score: 0.0011487650775416428\n",
      "Validation Accuracy: 0.790393539444344\n",
      "Validation Recall: 0.6666666666666666\n",
      "Validation Precision: 0.0005748778384593274\n",
      "Validation AUC: 0.858006831424553\n",
      "Epoch 6\n",
      "Average training loss: 8.924269841705476e-07\n",
      "Training F1 Score: 0.0005259982363588545\n",
      "Training Accuracy: 0.6070613995073442\n",
      "Training Recall: 0.6071428571428571\n",
      "Training Precision: 0.00026311309219792295\n",
      "Training AUC: 0.6266321459204716\n",
      "Validation Loss: 7.46257148875884e-07\n",
      "Validation F1 Score: 0.00120312813314618\n",
      "Validation Accuracy: 0.699813174230097\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0006019261637239165\n",
      "Validation AUC: 0.8764315852923448\n",
      "Epoch 7\n",
      "Average training loss: 8.677102684720101e-07\n",
      "Training F1 Score: 0.000543591219403009\n",
      "Training Accuracy: 0.6197852993948241\n",
      "Training Recall: 0.6071428571428571\n",
      "Training Precision: 0.0002719173371295126\n",
      "Training AUC: 0.6708330472778088\n",
      "Validation Loss: 7.383648494396767e-07\n",
      "Validation F1 Score: 0.0011263375258119017\n",
      "Validation Accuracy: 0.6793226059181583\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005634861006761833\n",
      "Validation AUC: 0.8765521398432792\n",
      "Epoch 8\n",
      "Average training loss: 8.541679242503827e-07\n",
      "Training F1 Score: 0.0006169031462060457\n",
      "Training Accuracy: 0.6255816075175623\n",
      "Training Recall: 0.6785714285714286\n",
      "Training Precision: 0.0003085918466785772\n",
      "Training AUC: 0.6906066173115879\n",
      "Validation Loss: 7.036601800791687e-07\n",
      "Validation F1 Score: 0.0012978585334198572\n",
      "Validation Accuracy: 0.7217501355993491\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0006493506493506494\n",
      "Validation AUC: 0.9065099457504521\n",
      "Epoch 9\n",
      "Average training loss: 8.537598079129101e-07\n",
      "Training F1 Score: 0.0005437573633809624\n",
      "Training Accuracy: 0.5975428032722075\n",
      "Training Recall: 0.6428571428571429\n",
      "Training Precision: 0.000271993713923056\n",
      "Training AUC: 0.6825987282623147\n",
      "Validation Loss: 6.894702210047399e-07\n",
      "Validation F1 Score: 0.0012244897959183673\n",
      "Validation Accuracy: 0.7050563490628579\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.000612619971411068\n",
      "Validation AUC: 0.9236889692585896\n",
      "Epoch 10\n",
      "Average training loss: 9.57583611624414e-07\n",
      "Training F1 Score: 0.0006954344726868111\n",
      "Training Accuracy: 0.6504090259404556\n",
      "Training Recall: 0.7142857142857143\n",
      "Training Precision: 0.00034788658897199514\n",
      "Training AUC: 0.7214791054906149\n",
      "Validation Loss: 6.563328042830108e-07\n",
      "Validation F1 Score: 0.0015544041450777201\n",
      "Validation Accuracy: 0.7677333815464352\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.00077780658542909\n",
      "Validation AUC: 0.9241109101868595\n",
      "Epoch 11\n",
      "Average training loss: 8.212181290762201e-07\n",
      "Training F1 Score: 0.000689849800884262\n",
      "Training Accuracy: 0.6123346409999088\n",
      "Training Recall: 0.7857142857142857\n",
      "Training Precision: 0.0003450763873639301\n",
      "Training AUC: 0.733062833435734\n",
      "Validation Loss: 6.601778011372516e-07\n",
      "Validation F1 Score: 0.0010828370330265296\n",
      "Validation Accuracy: 0.6664256011571145\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005417118093174431\n",
      "Validation AUC: 0.9380349608197709\n",
      "Epoch 12\n",
      "Average training loss: 8.907162398871861e-07\n",
      "Training F1 Score: 0.0005929928017262679\n",
      "Training Accuracy: 0.6309764924124928\n",
      "Training Recall: 0.6428571428571429\n",
      "Training Precision: 0.0002966332130320858\n",
      "Training AUC: 0.7277654472155862\n",
      "Validation Loss: 6.258387941215915e-07\n",
      "Validation F1 Score: 0.0011261261261261261\n",
      "Validation Accuracy: 0.6792623395407702\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005633802816901409\n",
      "Validation AUC: 0.9553345388788427\n",
      "Epoch 13\n",
      "Average training loss: 8.174040264814876e-07\n",
      "Training F1 Score: 0.0006149777839275556\n",
      "Training Accuracy: 0.6046406958002616\n",
      "Training Recall: 0.7142857142857143\n",
      "Training Precision: 0.0003076213181573483\n",
      "Training AUC: 0.7225438838142397\n",
      "Validation Loss: 6.056566348888736e-07\n",
      "Validation F1 Score: 0.0011476664116296864\n",
      "Validation Accuracy: 0.6852889772795757\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005741626794258373\n",
      "Validation AUC: 0.9530038175607796\n",
      "Epoch 14\n",
      "Average training loss: 8.101472535211697e-07\n",
      "Training F1 Score: 0.0006705783738474435\n",
      "Training Accuracy: 0.6374418392482438\n",
      "Training Recall: 0.7142857142857143\n",
      "Training Precision: 0.00033544664721076115\n",
      "Training AUC: 0.7255641956393841\n",
      "Validation Loss: 5.944131885371462e-07\n",
      "Validation F1 Score: 0.0011695906432748538\n",
      "Validation Accuracy: 0.6911950822636052\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005851375073142189\n",
      "Validation AUC: 0.9497287522603978\n",
      "Epoch 15\n",
      "Average training loss: 8.186457418515801e-07\n",
      "Training F1 Score: 0.0006541139092650563\n",
      "Training Accuracy: 0.6097253900191588\n",
      "Training Recall: 0.75\n",
      "Training Precision: 0.0003271996385223041\n",
      "Training AUC: 0.7154962722982091\n",
      "Validation Loss: 6.05974321388203e-07\n",
      "Validation F1 Score: 0.0010171215460247499\n",
      "Validation Accuracy: 0.6448502380521907\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005088195386702849\n",
      "Validation AUC: 0.9674703636728953\n",
      "Epoch 16\n",
      "Average training loss: 7.881304588031858e-07\n",
      "Training F1 Score: 0.0006524966151738088\n",
      "Training Accuracy: 0.6273880120426968\n",
      "Training Recall: 0.7142857142857143\n",
      "Training Precision: 0.0003263973888208894\n",
      "Training AUC: 0.7392474987160089\n",
      "Validation Loss: 5.51976586393721e-07\n",
      "Validation F1 Score: 0.0012404382881951624\n",
      "Validation Accuracy: 0.7088531308383053\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0006206040546131568\n",
      "Validation AUC: 0.9631705846895721\n",
      "Epoch 17\n",
      "Average training loss: 8.657239736360835e-07\n",
      "Training F1 Score: 0.0006382784717789732\n",
      "Training Accuracy: 0.6000364930207098\n",
      "Training Recall: 0.75\n",
      "Training Precision: 0.0003192750935019917\n",
      "Training AUC: 0.7268152721496052\n",
      "Validation Loss: 5.642036512654158e-07\n",
      "Validation F1 Score: 0.0010375237765865469\n",
      "Validation Accuracy: 0.651841137829205\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005190311418685121\n",
      "Validation AUC: 0.9739602169981916\n",
      "Epoch 18\n",
      "Average training loss: 7.776718804529437e-07\n",
      "Training F1 Score: 0.0006967022758941013\n",
      "Training Accuracy: 0.581254751695405\n",
      "Training Recall: 0.8571428571428571\n",
      "Training Precision: 0.0003484927687750479\n",
      "Training AUC: 0.7486209589044668\n",
      "Validation Loss: 5.408508656046252e-07\n",
      "Validation F1 Score: 0.001098297638660077\n",
      "Validation Accuracy: 0.6711263785933828\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005494505494505495\n",
      "Validation AUC: 0.9744022503516174\n",
      "Epoch 19\n",
      "Average training loss: 7.686537628615042e-07\n",
      "Training F1 Score: 0.0007273072240580581\n",
      "Training Accuracy: 0.6156007663534349\n",
      "Training Recall: 0.8214285714285714\n",
      "Training Precision: 0.00036381467596766797\n",
      "Training AUC: 0.735822545057004\n",
      "Validation Loss: 5.411198515048454e-07\n",
      "Validation F1 Score: 0.0010121457489878543\n",
      "Validation Accuracy: 0.6431025131079371\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005063291139240507\n",
      "Validation AUC: 0.9717902350813743\n",
      "Epoch 20\n",
      "Average training loss: 7.623737158100107e-07\n",
      "Training F1 Score: 0.0006457758189244604\n",
      "Training Accuracy: 0.6235075875072226\n",
      "Training Recall: 0.7142857142857143\n",
      "Training Precision: 0.0003230339347148418\n",
      "Training AUC: 0.7375108520051551\n",
      "Validation Loss: 5.259415587488547e-07\n",
      "Validation F1 Score: 0.0010303967027305513\n",
      "Validation Accuracy: 0.6494304827336829\n",
      "Validation Recall: 1.0\n",
      "Validation Precision: 0.0005154639175257732\n",
      "Validation AUC: 0.9711271850512356\n",
      "Early stopping!\n",
      "Best Epoch is: 10\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    model.train()\n",
    "    all_train_mro_preds = []\n",
    "    all_train_mro_targets = []\n",
    "    all_train_mro_scores = []\n",
    "    for train_inputs, train_targets, train_lengths in train_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_inputs = train_inputs.to(device)\n",
    "        train_targets = train_targets.to(device)\n",
    "        # train_lengths = train_lengths.to(device)\n",
    "\n",
    "        model_out = model(train_inputs, train_lengths)\n",
    "\n",
    "        loss = criterion(model_out, train_targets[:, -1, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        mro_pred = torch.sigmoid(model_out)\n",
    "        mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "        mro_targets = train_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "        all_train_mro_preds.extend(mro_preds)\n",
    "        all_train_mro_targets.extend(mro_targets)\n",
    "        all_train_mro_scores.extend(\n",
    "            torch.sigmoid(model_out).detach().cpu().numpy().flatten()\n",
    "        )\n",
    "\n",
    "    average_loss = running_loss / len(train_dataloader)\n",
    "    print(f\"Average training loss: {average_loss}\")\n",
    "\n",
    "    train_f1 = f1_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training F1 Score: {train_f1}\")\n",
    "    train_accuracy = accuracy_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    train_recall = recall_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Recall: {train_recall}\")\n",
    "    train_precision = precision_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Precision: {train_precision}\")\n",
    "    train_auc = roc_auc_score(all_train_mro_targets, all_train_mro_scores)\n",
    "    print(f\"Training AUC: {train_auc}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_mro_preds = []\n",
    "    all_val_mro_targets = []\n",
    "    all_val_mro_scores = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets, test_lengths in test_dataloader:\n",
    "            val_inputs = val_inputs.to(device)\n",
    "            val_targets = val_targets.to(device)\n",
    "            model_out = model(val_inputs, test_lengths)\n",
    "            loss = criterion(model_out, val_targets[:, -1, :])\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            mro_pred = torch.sigmoid(model_out)\n",
    "            mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "            mro_targets = val_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "            all_val_mro_preds.extend(mro_preds)\n",
    "            all_val_mro_targets.extend(mro_targets)\n",
    "            all_val_mro_scores.extend(\n",
    "                torch.sigmoid(model_out).detach().cpu().numpy().flatten()\n",
    "            )\n",
    "        average_val_loss = val_loss / len(test_dataloader)\n",
    "        print(f\"Validation Loss: {average_val_loss}\")\n",
    "        val_f1 = f1_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation F1 Score: {val_f1}\")\n",
    "        val_accuracy = accuracy_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        val_recall = recall_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Recall: {val_recall}\")\n",
    "        val_precision = precision_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Precision: {val_precision}\")\n",
    "        val_auc = roc_auc_score(all_val_mro_targets, all_val_mro_scores)\n",
    "        print(f\"Validation AUC: {val_auc}\")\n",
    "\n",
    "    log_entry = {\n",
    "        \"Epoch\": epoch,\n",
    "        \"Train Loss\": average_loss,\n",
    "        \"Train F1\": train_f1,\n",
    "        \"Train Accuracy\": train_accuracy,\n",
    "        \"Train Recall\": train_recall,\n",
    "        \"Train Precision\": train_precision,\n",
    "        \"Train AUC\": train_auc,\n",
    "        \"Val Loss\": average_val_loss,\n",
    "        \"Val F1\": val_f1,\n",
    "        \"Val Accuracy\": val_accuracy,\n",
    "        \"Val Recall\": val_recall,\n",
    "        \"Val Precision\": val_precision,\n",
    "        \"Val AUC\": val_auc,\n",
    "    }\n",
    "    entry_df = pd.DataFrame([log_entry])\n",
    "    entry_df.to_csv(log_file_path, mode=\"a\", header=False, index=False)\n",
    "\n",
    "    if best_val_f1 < val_f1:\n",
    "        best_val_f1 = val_f1\n",
    "        counter = 0\n",
    "        # save the model\n",
    "        torch.save(model.state_dict(), model_save_path)\n",
    "        # torchscript_model = torch.jit.script(model)\n",
    "        # torchscript_model.save(pt_model_save_path)\n",
    "        best_epoch = epoch\n",
    "        best_log_entry = log_entry\n",
    "\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter >= patience:\n",
    "            print(\"Early stopping!\")\n",
    "            print(\"Best Epoch is:\", best_epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a937a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
