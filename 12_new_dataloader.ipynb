{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13ed8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import (\n",
    "    f1_score,\n",
    "    accuracy_score,\n",
    "    recall_score,\n",
    "    precision_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "from utils import create_train_test_group\n",
    "from utils import collate_fn\n",
    "\n",
    "from model import FocalLoss\n",
    "from model import RnnModel\n",
    "from model import mroRnnDataset\n",
    "\n",
    "# Check if CUDA (NVIDIA GPU) is available\n",
    "if torch.cuda.is_available():\n",
    "    # Get the number of available CUDA devices\n",
    "    num_gpus = torch.cuda.device_count()\n",
    "    print(f\"Number of CUDA devices available: {num_gpus}\")\n",
    "    # Select a specific GPU (e.g., GPU 0)\n",
    "    device = torch.device(\"cuda:6\")  # Use \"cuda:1\" for GPU 1, etc.\n",
    "    print(f\"Using device: {torch.cuda.get_device_name(device)}\")\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(\"CUDA is not available, using CPU\")\n",
    "print(f\"Using {device} device\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83441a13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "# data preprocessing control parameter\n",
    "sample_frac = 1\n",
    "test_size = 0.1\n",
    "valid_size = 0.1\n",
    "max_seq_length = 8\n",
    "batch_size = 4096\n",
    "num_workers = 16\n",
    "\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 1000\n",
    "\n",
    "\n",
    "best_val_loss = float(\"inf\")\n",
    "best_val_auc = 0.0\n",
    "best_val_f1 = 0.0\n",
    "\n",
    "best_epoch = float(\"inf\")\n",
    "counter = 0\n",
    "patience = 20\n",
    "\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# Parameters for RNN Model\n",
    "rnn_type = \"LSTM\"\n",
    "rnn_output_size = 128\n",
    "bidirectional = True\n",
    "# pooling_method value =  None, 'max', 'avg', 'attention', 'multihead_attention'\n",
    "num_layers = 2\n",
    "pooling_method = None\n",
    "num_heads = None\n",
    "use_last_hidden = True\n",
    "agg_fun = [\"mean\", \"sum\", \"max\", \"min\", \"std\", \"skew\"]\n",
    "\n",
    "\n",
    "file_name = \"./Data/mro_daily_clean.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2255e951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "data = pd.read_csv(file_name, index_col=0, engine=\"pyarrow\")\n",
    "\n",
    "\n",
    "continuous_variable = [\n",
    "    \"hard_braking\",\n",
    "    \"hard_acceleration\",\n",
    "    \"speeding_sum\",\n",
    "    \"day_mileage\",\n",
    "    \"engn_size\",\n",
    "    \"est_hh_incm_prmr_cd\",\n",
    "    \"purchaser_age_at_tm_of_purch\",\n",
    "    \"tavg\",\n",
    "    \"random_avg_traffic\",\n",
    "]\n",
    "\n",
    "category_variable = [\n",
    "    \"gmqualty_model\",\n",
    "    \"umf_xref_finc_gbl_trim\",\n",
    "    \"input_indiv_gndr_prmr_cd\",\n",
    "]\n",
    "\n",
    "driver_navigation = [\n",
    "    \"id\",\n",
    "    \"yr_nbr\",\n",
    "    \"mth_nbr\",\n",
    "    \"week_nbr\",\n",
    "]\n",
    "\n",
    "mro = [\"mro\"]\n",
    "\n",
    "\n",
    "data = data[driver_navigation + continuous_variable + category_variable + mro]\n",
    "\n",
    "\n",
    "data = data.groupby([\"id\", \"yr_nbr\", \"week_nbr\"]).agg(\n",
    "    {\n",
    "        \"mth_nbr\": \"first\",\n",
    "        \"mro\": \"max\",\n",
    "        \"hard_braking\": [\"mean\", \"max\", \"min\", \"std\", \"skew\"],\n",
    "        \"hard_acceleration\": [\"mean\", \"max\", \"min\", \"std\", \"skew\"],\n",
    "        \"speeding_sum\": [\"mean\", \"max\", \"min\", \"std\", \"skew\"],\n",
    "        \"day_mileage\": [\"mean\", \"max\", \"min\", \"std\", \"skew\"],\n",
    "        \"est_hh_incm_prmr_cd\": \"first\",\n",
    "        \"purchaser_age_at_tm_of_purch\": \"first\",\n",
    "        \"input_indiv_gndr_prmr_cd\": \"first\",\n",
    "        \"gmqualty_model\": \"first\",\n",
    "        \"umf_xref_finc_gbl_trim\": \"first\",\n",
    "        \"engn_size\": \"first\",\n",
    "        \"tavg\": agg_fun,\n",
    "        \"random_avg_traffic\": agg_fun,\n",
    "    }\n",
    ")\n",
    "\n",
    "data.reset_index(inplace=True)\n",
    "\n",
    "\n",
    "def flatten_columns(df: pd.DataFrame):\n",
    "    def clean_col(col):\n",
    "        if isinstance(col, tuple):\n",
    "            col_name, agg_func = col\n",
    "            agg_func = agg_func.strip()\n",
    "            if col_name in mro and agg_func == \"max\":\n",
    "                return \"mro\"\n",
    "            if agg_func in (\"first\", \"\"):\n",
    "                return col_name\n",
    "            return f\"{col_name}_{agg_func}\"\n",
    "        else:\n",
    "            return col\n",
    "\n",
    "    df.columns = [clean_col(col) for col in df.columns]\n",
    "    return df\n",
    "\n",
    "\n",
    "data = flatten_columns(data)\n",
    "data.fillna(0, inplace=True)\n",
    "data = data.drop([\"yr_nbr\", \"week_nbr\", \"mth_nbr\"], axis=1)\n",
    "\n",
    "\n",
    "col_need_std = [\n",
    "    item\n",
    "    for item in data.columns.values.tolist()\n",
    "    if item not in (mro + [\"id\"] + category_variable)\n",
    "]\n",
    "\n",
    "col_need_encode = category_variable\n",
    "\n",
    "\n",
    "scaler = StandardScaler()\n",
    "data[col_need_std] = scaler.fit_transform(data[col_need_std])\n",
    "\n",
    "\n",
    "encoder = OneHotEncoder(sparse_output=False)\n",
    "encoded_categorical = encoder.fit_transform(data[col_need_encode])\n",
    "\n",
    "category_counts = [len(encoder.categories_[i]) for i, _ in enumerate(col_need_encode)]\n",
    "\n",
    "onehot_feature_names = []\n",
    "for col_idx, col in enumerate(col_need_encode):\n",
    "    num_categories = category_counts[col_idx]\n",
    "    onehot_feature_names.extend([f\"{col}_onehot_{i}\" for i in range(num_categories)])\n",
    "\n",
    "encoded_df = pd.DataFrame(\n",
    "    encoded_categorical, index=data.index, columns=onehot_feature_names\n",
    ")\n",
    "data = pd.concat([data, encoded_df], axis=1)\n",
    "\n",
    "\n",
    "rnn_features = col_need_std + onehot_feature_names\n",
    "# col_rnn_origin = [\"id\"] + rnn_features + [\"mro\"]\n",
    "col_rnn_origin = [\"id\"] + rnn_features + mro\n",
    "data_rnn_origin = data[col_rnn_origin].copy()\n",
    "data_rnn_origin = create_train_test_group(\n",
    "    data_rnn_origin,\n",
    "    sample_frac=sample_frac,\n",
    "    test_size=test_size,\n",
    "    valid_size=valid_size,\n",
    "    random_state=42,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5312fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# rnn_target = [\"mro\"]\n",
    "rnn_target = mro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0bd22de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------------------------------------------------------\n",
    "train_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"train\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "val_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"valid\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "test_data_set = mroRnnDataset(\n",
    "    data_rnn_origin=data_rnn_origin,\n",
    "    rnn_features=rnn_features,\n",
    "    rnn_target=rnn_target,\n",
    "    group=\"test\",\n",
    "    max_seq_length=max_seq_length,\n",
    ")\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "train_dataloader = DataLoader(\n",
    "    train_data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "val_dataloader = DataLoader(\n",
    "    val_data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")\n",
    "\n",
    "test_dataloader = DataLoader(\n",
    "    test_data_set,\n",
    "    batch_size=batch_size,\n",
    "    shuffle=False,\n",
    "    collate_fn=collate_fn,\n",
    "    num_workers=num_workers,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba4225d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.dataset[0][0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce2bb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader.dataset[0][1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9897a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "for train_inputs, train_targets, train_lengths in train_dataloader:\n",
    "    t_input = train_inputs[:, :-1, :]\n",
    "    t_output = train_targets[:, -1, :]\n",
    "    print(t_input.shape)\n",
    "    print(t_output.shape)\n",
    "    print(len(train_lengths))\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb237ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_feature_size = len(rnn_features)\n",
    "output_size = len(rnn_target)\n",
    "\n",
    "\n",
    "model = RnnModel(\n",
    "    rnn_type=rnn_type,\n",
    "    input_size=input_feature_size,\n",
    "    rnn_output_size=rnn_output_size,\n",
    "    output_size=output_size,\n",
    "    bidirectional=bidirectional,\n",
    "    num_layers=num_layers,\n",
    "    # value =  None, 'max', 'avg', 'attention', 'multihead_attention'\n",
    "    pooling_method=pooling_method,\n",
    "    num_heads=num_heads,\n",
    "    use_last_hidden=True,\n",
    ").to(device)\n",
    "\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae6e5e90",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bd2108e",
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 1 - data_rnn_origin[\"mro\"].eq(1).mean()\n",
    "print(f\"Alpha value for Focal Loss: {alpha}\")\n",
    "gamma = 4\n",
    "print(f\"Gamma value for Focal Loss: {gamma}\")\n",
    "criterion = FocalLoss(alpha=alpha, gamma=gamma).to(device)\n",
    "\n",
    "# optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=1e-4)\n",
    "# add learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, \"min\", patience=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a92866",
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    train_running_loss = 0.0\n",
    "    print(f\"Epoch {epoch}\")\n",
    "    model.train()\n",
    "    all_train_mro_preds = []\n",
    "    all_train_mro_targets = []\n",
    "    all_train_mro_scores = []\n",
    "    for train_inputs, train_targets, train_lengths in train_dataloader:\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        train_inputs = train_inputs[:, :-1, :].to(device)\n",
    "        \n",
    "        # train_inputs = train_inputs[:, :-1, :]\n",
    "        # train_inputs = train_inputs.to(device)\n",
    "        train_targets = train_targets.to(device)\n",
    "        # train_lengths = train_lengths.to(device)\n",
    "\n",
    "        # model_out = model(train_inputs, train_lengths)\n",
    "        model_out = model(train_inputs, train_lengths)\n",
    "        # the loss using Focal Loss with mean\n",
    "        # which means the average loss of this batch\n",
    "        loss = criterion(model_out, train_targets[:, -1, :])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # running loss equals to single loss * (train length / batch_szie)\n",
    "        train_running_loss += loss.item()\n",
    "\n",
    "        mro_pred = torch.sigmoid(model_out)\n",
    "        mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "        mro_targets = train_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "        all_train_mro_preds.extend(mro_preds)\n",
    "        all_train_mro_targets.extend(mro_targets)\n",
    "        all_train_mro_scores.extend(\n",
    "            torch.sigmoid(model_out).detach().cpu().numpy().flatten()\n",
    "        )\n",
    "\n",
    "    average_loss = train_running_loss / (len(train_dataloader) / batch_size)\n",
    "    print(f\"Average training loss: {average_loss}\")\n",
    "\n",
    "    train_f1 = f1_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training F1 Score: {train_f1}\")\n",
    "    train_accuracy = accuracy_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Accuracy: {train_accuracy}\")\n",
    "    train_recall = recall_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Recall: {train_recall}\")\n",
    "    train_precision = precision_score(all_train_mro_targets, all_train_mro_preds)\n",
    "    print(f\"Training Precision: {train_precision}\")\n",
    "    train_auc = roc_auc_score(all_train_mro_targets, all_train_mro_scores)\n",
    "    print(f\"Training AUC: {train_auc}\")\n",
    "\n",
    "    # Validation loop\n",
    "    model.eval()\n",
    "    val_loss = 0.0\n",
    "    all_val_mro_preds = []\n",
    "    all_val_mro_targets = []\n",
    "    all_val_mro_scores = []\n",
    "    with torch.no_grad():\n",
    "        for val_inputs, val_targets, val_lengths in val_dataloader:\n",
    "            val_inputs = val_inputs[:, :-1, :].to(device)\n",
    "            val_targets = val_targets.to(device)\n",
    "            model_out = model(val_inputs, val_lengths)\n",
    "            loss = criterion(model_out, val_targets[:, -1, :])\n",
    "\n",
    "            val_loss += loss.item()\n",
    "            mro_pred = torch.sigmoid(model_out)\n",
    "            mro_preds = (mro_pred > 0.5).int().cpu().numpy().flatten()\n",
    "            mro_targets = val_targets[:, -1, :].cpu().numpy().flatten()\n",
    "\n",
    "            all_val_mro_preds.extend(mro_preds)\n",
    "            all_val_mro_targets.extend(mro_targets)\n",
    "            all_val_mro_scores.extend(\n",
    "                torch.sigmoid(model_out).detach().cpu().numpy().flatten()\n",
    "            )\n",
    "\n",
    "        average_val_loss = val_loss / (len(val_dataloader) / batch_size)\n",
    "        print(f\"Validation Loss: {average_val_loss}\")\n",
    "        # update learning rate with scheduler\n",
    "        scheduler.step(average_val_loss)\n",
    "\n",
    "        val_f1 = f1_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation F1 Score: {val_f1}\")\n",
    "        val_accuracy = accuracy_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Accuracy: {val_accuracy}\")\n",
    "        val_recall = recall_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Recall: {val_recall}\")\n",
    "        val_precision = precision_score(all_val_mro_targets, all_val_mro_preds)\n",
    "        print(f\"Validation Precision: {val_precision}\")\n",
    "        val_auc = roc_auc_score(all_val_mro_targets, all_val_mro_scores)\n",
    "        print(f\"Validation AUC: {val_auc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a11ede3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mro",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
